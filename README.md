# Phase 1
 
- **D√©finir les objectifs et m√©triques du projet**
- **Mettre en place l'environnement de d√©veloppement avec Docker ou de simples environnements virtuels**
- **Cr√©er les premiers pipelines de donn√©es et d'entra√Ænement**
- **Mise en place Git pour le code et premiers tests automatis√©s**
- **Cr√©er une API basique pour le mod√®le**
## Cadrage du Projet MLOps
Cataloguer les produits selon des donn√©es diff√©rentes (textes et images) est important pour les e-commerces puisque cela permet de r√©aliser des applications diverses telles que la recommandation de produits et la recherche personnalis√©e. Il s‚Äôagit alors de pr√©dire le code type des produits √† partir de donn√©es textuelles (d√©signation et description des produits) ainsi que des donn√©es d'images (image du produit).
Le client est le site internet de Rakuten, et plus particuli√®rement les administrateurs de ce site.
### √âtapes pour cadrer le projet MLOps (Rakuten)
1. **Objectifs et Probl√©matique**
   1. Pourquoi mettre en place un pipeline MLOps ? (automatisation, scalabilit√©, fiabilit√©‚Ä¶)
   2. Quels sont les objectifs ?
      1. Construire un mod√®le de classification d‚Äôimages pour cat√©goriser des produits e-commerce.
      2. D√©ploiement du mod√®le deep learning en production.
   3. Quels sont les KPIs pour mesurer la performance du mod√®le et du pipeline MLOps ? (ex. : pr√©cision, latence, co√ªt d‚Äôinf√©rence‚Ä¶)
      4. M√©triques :
         5. Accuracy : √âvaluer la proportion de bonnes pr√©dictions 
         6. F1-score : Prendre en compte le d√©s√©quilibre des classes 
         7. Temps d‚Äôinf√©rence : Mesurer la rapidit√© des pr√©dictions du mod√®le
2. **Donn√©es et Infrastructure**
   1. **Sources des donn√©es** : stock√©s dans HuggingFace: https://huggingface.co/datasets/ndiguesene/ml-datasets-image-rakuten-ecommerce/resolve/main/
      - Donn√©es textuelles (~60 MB)
      - Donn√©es images (~2.5 GB)
   2. **Stockage** : fichiers plats (CSV)
   3. **Nettoyage & Pr√©paration** : Strat√©gie de gestion des donn√©es manquantes et transformation.
   4. **Infrastructure** : √Ä d√©finir.
3. **D√©veloppement du Mod√®le ML**
   1. **Choix des algorithmes** : Deep learning
   2. **Frameworks utilis√©s** : TensorFlow, Scikit-learn
4. **Industrialisation avec MLOps**
   1. **CI/CD pour le ML** : GitHub, GitHub Actions
   2. **Gestion des mod√®les** : MLflow
   3. **Orchestration** : Airflow
   4. **Monitoring & Observabilit√©** : Prometheus, Grafana
5. **D√©ploiement et Scalabilit√©**
   1. **Mode de d√©ploiement** : Batch (REST API)
   2. **Infrastructure de d√©ploiement** : Docker, Kubernetes (√† confirmer)
   3. **Gestion du drift** : Retrain automatique, A/B testing, monitoring des performances
6. **S√©curit√© et Gouvernance**
   1. S√©curis√© les APIs expos√©es
## Mettre en place l'environnement de d√©veloppement reproductible
### Cr√©ation de l'environnement 
#### - Sans Docker
Cr√©ez l'environnement virtuel avec les √©tapes suivantes sur Linux :
 
`python -m venv rakuten-project-mlops`
 
Cela va cr√©er un dossier `rakuten-project-mlops/` contenant l‚Äôenvironnement virtuel.
 
### Installer les d√©pendances
 
Installez les d√©pendances n√©cessaires :
 
`pip install -r requirements.txt`
 
#### - Avec Docker
- Cr√©er un Dockerfile et un `docker-compose.yml`
- Construire l‚Äôimage avec `docker build -t ml_env .`
- Lancer le conteneur avec `docker-compose up`
### Importer les donn√©es dans `raw/data`
 
Ensuite, ex√©cutez le script pour importer les donn√©es depuis le repo HuggingFace :
`python3 src/data/import_raw_data.py`
 
Arborescence des donn√©es
```
    data/raw/
    ‚îÇ‚îÄ‚îÄ image_train/
    ‚îÇ   ‚îú‚îÄ‚îÄ image_123456789_product_987654321.jpg
    ‚îÇ   ‚îú‚îÄ‚îÄ image_987654321_product_123456789.jpg
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îÇ‚îÄ‚îÄ image_test/
    ‚îÇ   ‚îú‚îÄ‚îÄ image_111111111_product_222222222.jpg
    ‚îÇ   ‚îú‚îÄ‚îÄ image_222222222_product_111111111.jpg
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
```
---
## Copiez les donn√©es brutes dans `data/preprocessed/` :
 
`python3 src/data/make_dataset.py data/raw data/preprocessed`
 
## Entra√Ænez les mod√®les sur l‚Äôensemble de donn√©es et enregistrez-les dans le dossier models
 
`python3 python src/main.py`
## D√©marrer les pratiques de versionning et des tests automatis√©s
### Suivi et gestion des versions des donn√©es et des mod√®les de donn√©es
1. **Initialiser DVC dans le projet**
```bash
  dvc init
```
2. **Ajouter les fichiers volumineux avec DVC**
```bash
    dvc add data/raw/image_train
    dvc add data/raw/image_test
    dvc add data/preprocessed/X_train_update.csv
    dvc add data/preprocessed/X_test_update.csv
```
3. **Cr√©er un fichier `.gitignore` pour ignorer les fichiers volumineux suivis par DVC**
```bash
  echo "*.dvc" > .gitignore
```
### Configurer un stockage distant avec DVC
On utilisera **S3 d‚ÄôAWS** pour sauvegarder les donn√©es. Pour cela, il faut d'abord installer S3 √† l‚Äôaide de la commande suivante :

 
`pip install "dvc[S3]"`
 
 
Une fois S3 install√©, il faudra mettre √† jour le fichier de configuration `.dvc/config` pour ajouter DagsHub comme notre stockage distant.
---
## Impl√©menter une API basique

PYTHONPATH=$(pwd) pytest tests/test_api.py



---
## Impl√©menbtation d'une API basique


## Partie 1 : serving, dockerisation et tests unitaires
### Restructuration des r√©pertoires et fichiers

#### data : build
Ici, nous regroupons les logiques de r√©cup√©ration des donn√©es depuis le d√©p√¥t distant. Les donn√©es structur√©es sont r√©cup√©r√©es depuis AWS DataScientest. Les donn√©es d'images sont r√©cup√©r√©es depuis HiggingFace.

#### train
Ici, nous entra√Ænons le mod√®le (contient les fichiers main.py et train_model.py). Le r√©pertoire contiendra un Dockerfile sp√©cifique pour lancer l'entra√Ænement du mod√®le √† la demande. Les r√©sultats seront enregistr√©s dans `models` √† la racine

#### predict 
Ici, on applique le mod√®le au donn√©es d'entra√Ænement (contient le fichier predict.py). Le r√©pertoire contiendra un Dockerfile sp√©cifique pour lancer le test du mod√®le sur les donn√©es d'entra√Ænement; Les r√©sultats seront enregistr√©s sur data/predictions √† la racine

#### serve
Ici, on utilisera dans un premier temps requests ou FastAPI pour cr√©er une API qui va interroger le mod√®le avec des donn√©es pr√©alablement renseign√©es (fichier serve.py √† cr√©er) et retournera des r√©sultats que l'on pourra mettre dans data/predictions √† la racine avec un nom de fichier sp√©cifique
Niveaux de serving : 
1. Usage de FastAPI pour cr√©er un contneur basique de serving
2. Usage de BentoML pour automatiser le d√©ploiement, serving etc

#### Variables d'environnement : vous devez rajouter un fichier .env sur la racine du projet
Les chemins des mod√®les (poids, mapper etc) ne sont plus cod√©s en dur. Les modifications suivantes ont √©t√© apport√©es : 
1. ajout fichier `.env` contenant les variables d'environnement (**ce fichier ne doit pas √™tre pouss√© sur le git, il peut renfermer des informations sensibles tels cl√©s d'api etc**)
2. ajout fichier `.env.example` sur le git : fichier d'exemple montrant comment renseigner le fichier .env ( quelles sont les variables d'environnement etc)

## Conteneurisation
Docker est utilis√© pour conteneuriser les diff√©rents services de l'application. Chaque service dispose d'un r√©pertoire sp√©cifique sur les sources ainsi qu'un Dockerfile sp√©cifique.

### Variables d'environnement
Le fichier `.env` √† la racine du projet, doit √™tre mis en jour avec les r√©pertoires de volume cr√©√©s dans le conteneur (Exemple : `TOKENIZER_CONFIG_PATH=/app/models/tokenizer_config.json`).

### Services applicatifs
1. Service de donn√©es
Ici, le service effectue le t√©l√©chargement des donn√©es depuis le d√©p√¥t HuggingFace : `https://huggingface.co/datasets/ndiguesene/ml-datasets-image-rakuten-ecommerce/resolve/main/`.

`Package` : src/data/build_data/

2. Service d'entra√Ænement

`Package` : src/models/train
Contient les fichiers : 
- .dockerignore : fichier pr√©cisant les fichiers et r√©pertoires √† ignorer lors de la construction de l'image
- Dockerfile : commandes pour cr√©er l'image Docker pour ce service uniquement
- main.py : script principal contenant la logique d'entrainement global 
- train_model.py  : script contenant la logique d'entrainement des mod√®les
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les d√©pendences requis pour cr√©er l'image 

3. Service de test

Ici, nous utilisons les donn√©es d'entra√Ænement pour tester l'usage du mod√®le.
`Package` : src/models/predict
Contient les fichiers : 
- .dockerignore : fichier pr√©cisant les fichiers et r√©pertoires √† ignorer lors de la construction de l'image
- Dockerfile : commandes pour cr√©er l'image Docker pour ce service uniquement
- predic.py : script principal contenant la logique de test du mod√®le en utilisant le mod√®le entra√Æn√© et sauvegard√© sur le service d'entra√Ænement
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les d√©pendences requis pour cr√©er l'image 

4. Service de serving

Ici, nous cr√©ons une application permettant d'int√©rroger le mod√®le via API en utilisant FastAPI.
`Package` : src/models/serve
Contient les fichiers : 
- .dockerignore : fichier pr√©cisant les fichiers et r√©pertoires √† ignorer lors de la construction de l'image
- Dockerfile : commandes pour cr√©er l'image Docker pour ce service uniquement
- predic_logic.py : wrapper cr√©√© pour contenir les fonctions requis pour effectuer les pr√©dictions (traitement de l'image, traitement des donn√©es textuelles)
- serve_model_fastapi.py : script principal contenant l'application FastAPI (les endpoints de pr√©diction)
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les d√©pendences requis pour cr√©er l'image

### API de pr√©diction
Le service de pr√©diction est compos√© pour le moment de deux endpoints : 
- `/status` : pour obtenir le statut de l'application. Retourne un message si l'application est up.
- `/predict` :  pour obtenir une classifiction d'un code produit en fonction des informations suivantes : </br>
`product_identifier`: str = Form(...), # Un identifiant entier pour le produit. Cet identifiant est utilis√© pour associer le produit √† son code de type de produit correspondant. </br>
`designation`: str = Form(...), # Le titre du produit, un court texte r√©sumant le produit </br>
`description`: str = Form(...),  # Un texte plus d√©taill√© d√©crivant le produit. Tous les commer√ßants n'utilisent pas ce champ, donc, afin de conserver l‚Äôoriginalit√© des donn√©es, le champ description peut contenir des valeurs NaN pour de nombreux produits. </br>
`product_id`: str = Form (...), # Un identifiant unique pour ce produit </br>
`imageid`: str = Form (...), # Un identifinat unique de l'image associ√© √† ce produit. </br>
`image`: UploadFile = File(...)  # L'image correspondant au produit </br>

![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_input.png)
![Retour /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_return.png)

### Services : s√©curisation et logging

##### D√©couplage en services
Chaque fonctionnalit√© du syst√®me (t√©l√©chargement et traitement des donn√©es, entra√Ænement du mod√®le concat√©n√©, test du mod√®le entra√Æn√© et serving) est d√©fini en service respectivement situ√©s dans les r√©pertoires : src/data, src/models/train, src/models/predict, src/models/serve.
Chaque service est utilisable de fa√ßon d√©coupl√© via des conteneurs Docker ainsi que tous les packages requis pour son bon fonctionnement.

##### Limitation taux de requ√™te
La limitation du d√©bit permet d'√©viter les abus et de garantir une utilisation √©quitable de l'API.
La limite de 10 requ√™tes par minutes sur /predict est appliqu√©e.
Un gestionnaire d'exception global pour RateLimitExceeded garantit que les utilisateurs re√ßoivent un message d'erreur clair lorsque les limites sont d√©pass√©es.

##### Authentification et authorisation
Nous avons mis en ≈ìuvre OAuth2 avec JWT pour s√©curiser les points de terminaison. Les cl√©s d'authenication sont tous sauvegard√©s comme variables d'environnement.
Le point de terminaison `/token` g√©n√®re des jetons d'acc√®s, et la d√©pendance `get_current_user` garantit que seuls les utilisateurs authentifi√©s peuvent acc√©der aux points de terminaison prot√©g√©s comme `/predict`.
La protection des points de terminaison sensibles comme `/predict` garantit que seuls les utilisateurs autoris√©s peuvent acc√©der aux mod√®les. 
La logique de s√©curit√© est s√©par√©e dans security_logic.py, ce qui rend la base de code plus modulaire et r√©utilisable.
Dans cette version, les utilisateurs ne sont pas dans une base de donn√©es.
- `security_logic.py` : wrapper contenant les logiques d'authentification et d'authorisation utilis√©s par le script principal.

##### Logging
Nous avons mis en place un intergiciel qui enregistre chaque demande avec un identifiant unique, une m√©thode, une URL, un code d'√©tat et un temps de traitement.
La journalisation est essentielle pour le d√©bogage, la surveillance et l'audit.
L'inclusion des identifiants des requ√™tes et des temps de traitement facilite le suivi et l'optimisation des performances. 

- `import_data_logger.log`
- `train_model_logger.log`
- `test_model_logger.log`
- `serving_logger.log`

Nous avons inclu un (RotateFileHandler) pour g√©rer la limitation de la taille des fichiers de logs.

##### Chargement des mod√®les au d√©marrage
Nous avons rajout√© une m√©thode au d√©marrage de l'application de pr√©diction pour charger tous les mod√®les et fichiers de configuration pour √©viter leur chargement √† chaque demande de pr√©diction.

##### Orchestration
Le fichier `docker-compose.xml` √† la racine du projet est utilis√© pour orchestrer tous les services du projet. Nous utilisons le fichier .env pour assurer la r√©usabilit√© des diff√©rents environnemenst (production, staging etc) et la modularit√©.

##### Technologies utilis√©es
1. Docker : utilis√© pour cr√©er chaque service de l'application en un conteneur ind√©pendant
2. Logging et FileHandler : nous utilisons un logging centralis√© pour chaque service (t√©l√©chargement et traitement des donn√©es, entra√Ænement du mod√®le concat√©n√©, test du mod√®le entra√Æn√© et serving).

Chaque service retourne un fichier log sp√©cifique disponible dans le r√©pertoire logs/ √† la racine du projet.
3. Slowapi : utilis√© pour limiter le taux de requ√™te sur les routes.
4. Oauth2 et JWT : s√©curisation des routes d'API en terme d'authentification et d'authorisation.
5. bcrypt : encryption des informations utilisateurs (mots de passe) pour comparaison et validation des informations

##### Usage : 

- Demander un token : `/token`
`curl -X POST "http://127.0.0.1:8000/token" -d "username=user1&password=43SoYourAreADataEngineer34"`
Exemple retours : 
{
    "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
    "token_type": "bearer"
}
- Utiliser le token pour int√©rroger l'API de pr√©diction : `/predict_code`

![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/securing_apis/reports/securing_api_authorized.png)

---------

## Tests Unitaires
## Documentation des tests unitaires

Cette partie de la documentation d√©crit les tests unitaires d√©finis pour l‚ÄôAPI FastAPI. Ils couvrent les trois endpoints principaux : racine (`/`), `GET /status` et `POST /predict`.

---

## Pr√©requis

- Python 3.9
- FastAPI
- Uvicorn (pour lancer l‚ÄôAPI si besoin)
- pytest
- httpx (install√© automatiquement avec FastAPI)

Installer les d√©pendances :

```bash
  pip install fastapi uvicorn pytest httpx
```

## Lancer les tests

Depuis la racine du projet (l√† o√π se trouve src/ et tests/), ex√©cuter :

```bash
   pytest tests/test_api.py
```
## Structure des tests

Le fichier tests/test_api.py contient trois fonctions :
```python
from fastapi.testclient import TestClient
from Projet_Formation_MLOps_DataScientest_2025.api import app

client = TestClient(app)
```
Chaque test utilise un TestClient pour simuler des requ√™tes HTTP vers l‚Äôapplication FastAPI.

1. **test_status**
   2. Objectif : v√©rifier que l‚Äôendpoint GET /status renvoie bien un code HTTP 200 et le message attendu. 
   3. script
      ```python
      def test_status():
          response = client.get("/status")
          assert response.status_code == 200
          assert response.json() == {"status": "L'API fonctionne correctement"}
      ```
   3. Requ√™te : 
   ```http request
   GET /status
   ```
   4. Assertions :
      - `response.status_code == 200`
      - `response.json() == {"status": "L'API fonctionne correctement"}`

   2. **test_root**
      3. Objectif : valider que la racine / renvoie un message de bienvenue.
      4. script
      ```python
      def test_root():
          response = client.get("/")
          assert response.status_code == 200
          data = response.json()
          assert "message" in data
          assert "Bienvenue" in data["message"]
         ```
      4. Requ√™te :
      ```http request
      GET /
      ```
      5. Assertions
         - `response.status_code == 200`
         - `Le corps JSON contient une cl√© message`
         - `La cha√Æne "Bienvenue" appara√Æt dans la valeur de message`

3. **test_predict**
   4. Objectif : s‚Äôassurer que l‚Äôendpoint POST /predict accepte un payload JSON arbitraire et retourne un JSON avec les cl√©s predicted et label.
   5. script
   ```python
   def test_predict():
       sample_text = {"predicted": "1", "label": "label_1"}
       response = client.post("/predict", json=sample_text)
       assert response.status_code == 200
       data = response.json()
       assert "predicted" in data
       assert "label" in data
   ```
   5. Requ√™te :
   ```http request
   POST /predict
   Content-Type: application/json
   {"predicted": "1", "label": "label_1"}
   ```
   6. Assertions :
    - response.status_code == 200 
    - La r√©ponse JSON contient les champs predicted et label

---------

## Automatisation DVC/DgasHub/MLFlow

A partir de l√†, nous travaillons exclusvement avec Docker.
Donc, il faut obligatoirement disposer d'un fichier .env √† la racine du projet. Exemple de contenu .env disponible sur le fichier `.env.example``.

1. Apr√®s avoir cloner le code, cr√©er un fichier .env (ne pas oublier de mettre √† jour vos informations dagshub (token))
```bash
touch .env

```

2. Donn√©es n√©cessaires : fichiers artefacts non dispoinibles sur Git
Les fichiers .h5, .json, .pkl (best_lstm_model, best_vgg16, best_weights, mapper, tokenizer) doivent √™tre disponibles dans le dossier `models`.
Ils ont utilis√© dans l'entra√Ænement des mod√®les.
A la r√©cup√©ration du projet, si ces fichiers ne sont pas disponibles, faire une commande : 

```bash
dvc pull
```

3. Ex√©cuter la pipeline sans Airflow (tests uniquement)

```bash
docker compose up

```

!! Le fichier √† la racine `run_compose_options.sh` contient des examples de commandes pour ex√©cuter chaque service en particulier.

4. Ex√©cuter la pipeline via Airflow en mode standalone

Lancer une instance airflow en local (juste pour des tets) : mettre √† jour le fichier sous airflow/dags/initialize_airflow.sh et ex√©cuter le fichier

```bash
export AIRFLOW_HOME=`r√©pertoire absolu du projet`/airflow  
airflow db migrate

# D√©marrer airflow en mode standalone (cr√©e un utilisateur admin automatiquement, les credentials sont enregsitr√©s sous : airflow/simple_auth_manager_passwords.json.generated )
airflow standalone

```

5. Ex√©cuter la pipeline via Airflow et Docker
Dans cette partie du projet, nous entrons dans l'industrialisation.  Voir le point suivant.

---------

### Architecture MLOps
Pour automatiser l'ex√©cution des t√¢ches end-to-end, nous mettons en contribution Airflow(gestion de l'ordonnancement et ex√©cution automatique des t√¢ches), Docker(encapsulation des logiques avec toutes les d√©pendances) et DVC(gestionnaire du versionnage des donn√©es).

#### Motivations
Dans une pipeline de MLOps comme celui qui nous concerne, il est bien de s'assurer de certains aspects : 
- les donn√©es utilis√©es √† chaque ex√©cution du mod√®le sont connus et r√©cup√©rables, ceci rejoint l'aspect reproductibilit√©
- la version entra√Æn√©e du mod√®le est connu, suavegard√© et r√©cup√©rable, toujours sur l'aspect reproductibilit√©
- les artefacts du mod√®le correspondant √† cette version d'ex√©cution sont enregistr√©s et r√©cup√©rables
- la version g√©n√©rale du code est enregistr√©e et r√©cup√©rable
- les ex√©cutions sont agnostiques de la plateforme utilis√©e

#### Outils

- Airlfow nous permet de g√©rer l'ordonnancement et l'ex√©cution automatique des diff√©rentes t√¢ches requises pour entra√Æner le mod√®le
- MLfow nous permet de sauvegarder le mod√®le, les m√©triques ainsi que les artefacts produits apr√®s chaque entra√Ænement
- DVC nous permet d'assurer la sauvegarde et le versionning des donn√©es utilis√©es lors de l'entra√Ænement, le mod√®le produit, les fichiers artefacts produits
- Docker nous permet d'assurer l'encapsulation de touts les pr√©requis √† l'ex√©cution de tous les stages de notre pipeline d'entra√Ænement
- Git nous permet d'assurer la sauvegarde de tous les fichiers de code requis pour la bonne ex√©cution et la cr√©ation des images et conteneurs Docker

#### Configurations

##### Fichier .env
Le fichier .env est un fichier tr√®s important pour le projet se trouvant sur la racine. Il contient toutes les variables de configuration n√©cessaires pour la bonne ex√©cution de la pipeline MLFlow.
Il contient les liens vers les fichiers requis √† l'entra√Ænement du mod√®le (poids, tokenizer etc). Il contient √©galement, les r√©pertoires d'enregitrement des donn√©es ainsi que du mod√®le.
Ces derniers sont utilis√©s notamment par DVC pour le versionning des donn√©es.
Il contient √©galement les secret pour le module d'authentification.
Ce fichier n'est pas versionn√© mais un fichier exemple (`.enf.example`) est disponible sur le r√©pertoire distant.

##### DVC et MLflow
Nous utilisons DVC via DagsHub S3 en plus du tracking distant MLflow li√© √† DagsHub.
Les commandes de configuration DVC √† appliquer en local est disponible sur l'interface DagsHub.

Les configurations suivantes doivent √™tre renseign√©es sur le fichier .env
`MLFLOW_TRACKING_URI` : correspond au lien distant MLflow li√© √† DagsHub.
`MLFLOW_EXPERIMENT_NAME` : par ProductCodeClassifier
`DAGSHUB_USERNAME`: Nom d'utilisateur de votre compte DagsHub cr√©√©
`DAGSHUB_TOKEN` : Votre token DagsHub

Ici, nous d√©marrons la connexion √† MLflow distant (src/models/train/main.py) :

```python
with mlflow.start_run(run_name="Train_Concatenate_Model") as run:
  mlflow.set_tag("source", "airflow")
  mlflow.set_tag("version", run_time)
  mlflow.set_tag("model_type", "VGG16+LSTM")
```

Apr√®s entra√Ænement, les artefacts et le mod√®le sont sauvegard√©s :
```python
 # Log artefacts
  mlflow.log_artifact(tokenizer_config_path)
  mlflow.log_artifact(mapper_path_pkl)
  mlflow.log_artifact( best_weights_path_pkl)
  mlflow.log_artifact(BEST_WEIGHTS_PATH)
  mlflow.log_artifact(MAPPER_PATH)

  # Log the models
  mlflow.keras.log_model(lstm, "lstm_model")
  mlflow.keras.log_model(vgg16, "vgg16_model")
  mlflow.keras.log_model(concatenate_model, "concatenate_model")
```

##### Docker
Comme pr√©sent√© plus haut, les diff√©rents stages de notre pipeline sont dockeris√©s. Chaque stage dispose d'un Dockerfile permettant de cr√©er automatiquement une image.

##### Airflow
Nous utilisons Ariflow sous Docker, nous r√©cup√©rons donc le docker-compose officiel d√©di√© sur le site de Airflow (https://airflow.apache.org/docs/apache-airflow/3.0.4/docker-compose.yaml).
Ce fichier a √©t√© mis √† jour et est disponible √† la racine du projet : `docker-compose.yml`.
Il a √©t√© mis √† jour suivant : 
- les volumes : nous lions le projet en local aux diff√©rents conteneurs d√©marr√©s par Ariflow pour permettre la disponbilit√© des logiques et fichiers de code (` - ./:/opt/airflow/mlops-project `) n√©cessaires √† la bonne ex√©cution des stages en particulier des logiques pour cr√©er les images des stages (Dockerfile).
- l'utilisateur admin airflow

L'ex√©cution de  ce fichier produit les conteneurs/services suivants :  
- la base de  donn√©es PostgreSQL
- la base de donn√©es Redis
- le serveur d'api de Airflow
- le scheduler de Airflow
- le dag processor de Ariflow
- un worker
- un Triggerer

#### Stages de la pipeline MLflow

- D√©finition 
La d√©finition de la pipeline sous forme de dag Airflow est disponible sous `airflow/dags/ml_pipeline.py`.
Les op√©rateurs Docker : `BashOperator` et `DockerOperator` ont √©t√© utilis√©s.

Les diff√©rents stages de notre pipeline ont √©t√© regroup√©s et se pr√©sentent comme suit : 

![Pipeline global Airflow](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/automating_dvc_mlflow_docker/reports/pipeline.png)

1. R√©cup√©ration des donn√©es : 3 op√©rateurs Airflow
- construction de l'image Docker contenant la logique de r√©cup√©ration des donn√©es
- cr√©ation et lancement du conteneur de r√©cup√©ration des donn√©es
- enregistrement des donn√©es r√©cup√©r√©es sur DVC

2. Entra√Ænement du mod√®le : 3 op√©rateurs Airflow
- cr√©ation de l'image Docker contenant la logique d'entra√Ænement du mod√®le
- cr√©ation et lancement du conteneur d'entra√Ænement du mod√®le - enregistrement du mod√®le et des art√©facts sur MLflow
- enregistrement du mod√®le et des fichiers sur DVC

3. Test du mod√®le : 3 op√©rateurs Airflow
- cr√©ation de l'image de test
- cr√©ation et lancement du conteneur de test
- enregistrement des pr√©dictions r√©sultats sur DVC

4. Service d'exposition de l'API : 4 op√©rateurs Airflow
- cr√©ation image pour l'authentification
- cr√©ation et lancement du conteneur d'authentification : ce conteneur s'ex√©cute en backgroud apr√®s lancement
- cr√©ation de l'image d'exposition de l'API
- cr√©ation et lancement du conteneur d'exposition de l'API 

#### Lancement
- Premier lancement : initialise les bases Postgres SQL et Redis
```bash
make init-airflow
```
- Lancement : 
```bash
make start
```

![Pipeline global Airflow](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/automating_dvc_mlflow_docker/reports/detailled_pipeline.png)

#### Ports ouverts requis par la pipeline

Le service d'authentification est ouvert sur le port 8011:8011.
Le service d'API est ouvert sur le port 8000:8000.


###  Monitoring

Int√®grer un syst√®me de monitoring complet bas√© sur **Prometheus** et **Node Exporter**, avec exposition des m√©triques de l'application **FastAPI**.

## 1.  Installation des d√©pendances

Ajout de la d√©pendance Prometheus dans le fichier `requirements.txt` :

```
prometheus-fastapi-instrumentator
```

Installation avec pip :

```bash
pip install prometheus-fastapi-instrumentator
```

## 2.  Modification du code FastAPI

Ajout de l‚Äôinstrumentation Prometheus dans le fichier `src.models.serve.serve_model_fastapi.py` :

```python
from prometheus_fastapi_instrumentator import Instrumentator

# Ajout apr√®s la cr√©ation de l'application FastAPI
app = FastAPI()

# Instrumentation Prometheus
Instrumentator().instrument(app).expose(app)

```
## 3. Configuration prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  - job_name: "fastapi"
    static_configs:
      - targets: ["localhost:8000"]

  - job_name: "node"
    static_configs:
      - targets: ["localhost:9100"]


## 4.  Lancement de l‚ÄôAPI , Prometheus et node exporter

```bash
uvicorn src.models.serve.serve_model_fastapi:app --reload
```

```bash
prometheus --config.file=prometheus.yml
```

```bash

cd node_exporter-1.8.1.linux-amd64
./node_exporter &

```

## 5.  V√©rification de l‚Äôendpoint /metrics , prometheus, node exporter

Ouvrir dans le navigateur ou via curl :

```bash
http://localhost:8000/metrics

```
- > ![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_input.png)

```bash
//http://localhost:9090

```
- > ![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_input.png)

```bash
http://http://localhost:9100
```

## 6. M√©triques collect√©es (exemple) :
- `http_requests_total{job="fastapi"}` ‚Äî nombre de requ√™tes HTTP par endpoint, m√©thode, code
- `process_cpu_seconds_total` ‚Äî temps CPU utilis√©
- `node_memory_Active_bytes` ‚Äî RAM active

### ‚Üí Installer et configurer Grafana, le connecter √† Prometheus et visualiser les m√©triques (FastAPI, Node Exporter) avec des dashboards.
√âtape 1 : Lancer Grafana avec Docker

``` bash
docker run -d \
  -p 3000:3000 \
  --name=grafana \
  grafana/grafana

```
Demarrer grafana si d√©ja cr√©e
``` bash
docker start grafana
docker restart grafana

```
√âtape 2 : Ajouter Prometheus comme source de donn√©es
Acc√®de √† Grafana : http://localhost:3000(prochaine connexion docker start grafana)
Menu lat√©ral gauche ‚Üí ‚öôÔ∏è Configuration ‚Üí Data Sources
Clique sur Add data source
Choisis Prometheus
Dans le champ URL, mets :http://prometheus:9090

``` bash
 output
 Successfully queried the Prometheus API.
Next, you can start to visualize data by building a dashboard, or by querying data in the Explore view.
```

√âtape 3 : Importer un Dashboard Node Exporter + FastAPI
üîπ Option A : Dashboard Node Exporter (pr√™t √† l‚Äôemploi)
Menu gauche ‚Üí üìä Dashboards ‚Üí New Dashboards ‚Üí ADD visualisation ‚Üí choisir la source de donn√©es prometheus 
Menu gauche ‚Üí üìä Dashboards ‚Üí New Dashboards ‚Üí ADD visualisation ‚Üí choisir la source de donn√©es prometheus ‚Üí 

