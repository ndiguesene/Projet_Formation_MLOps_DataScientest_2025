- [Contexte et Objectifs](#contexte-et-objectifs)
  - [Objectifs et mÃ©triques](#objectifs-et-mÃ©triques)
- [DonnÃ©es et Infrastructure](#donnÃ©es-et-infrastructure)
- [Vous Ãªtes prÃ©ssÃ©s ? DÃ©marrez rapidement ici](#vous-%C3%AAtes-pr%C3%A9ss%C3%A9s--d%C3%A9marrez-rapidement-ici)
  - [Fichier .env](#fichier-env)
  - [DÃ©pendances](#dÃ©pendances)
  - [Configurer un stockage distant avec DVC](#configurer-un-stockage-distant-avec-dvc)
  - [RÃ©cupÃ©rer les artefacts et modÃ¨les et donnÃ©es dÃ©jÃ  existants](#rÃ©cupÃ©rer-les-artefacts-et-modÃ¨les-et-donnÃ©es-dÃ©jÃ -existants)
  - [Lancer la pipeline Airflow](#lancer-la-pipeline-airflow)
  - [Comment tester les endpoints](#comment-tester-les-endpoints)
- [Structure du projet](#structure-du-projet)
  - [Structure globale](#structure-globale)
  - [Struture du code des stages : data, train, auth, predict](#struture-du-code-des-stages--data-train-auth-predict)
- [DÃ©tails d'implÃ©mentation](#dÃ©tails-dimplÃ©mentation)
  - [Serving, dockerisation et tests unitaires](#serving-dockerisation-et-tests-unitaires)
  - [Conteneurisation](#conteneurisation)
  - [API de prÃ©diction](#api-de-prÃ©diction)
  - [Services : sÃ©curisation et logging](#services--sÃ©curisation-et-logging)
- [Tests Unitaires](#tests-unitaires)
  - [Documentation des tests unitaires](#documentation-des-tests-unitaires)
- [Automatisation Airflow/DVC/DagsHub/MLFlow](#automatisation-airflowdvcdagshubmlflow)
- [Monitoring](#monitoring)

# Contexte et Objectifs
 
## Objectifs et mÃ©triques
Ce projet ayant pour objectifs de cataloguer les produits selon des donnÃ©es diffÃ©rentes (textes et images) est important pour les e-commerces puisque cela permet de rÃ©aliser des applications diverses telles que la recommandation de produits et la recherche personnalisÃ©e. Il sâ€™agit alors de prÃ©dire le code type des produits Ã  partir de donnÃ©es textuelles (dÃ©signation et description des produits) ainsi que des donnÃ©es d'images (image du produit). Construire un modÃ¨le de classification dâ€™images pour catÃ©goriser des produits e-commerce.
Le client est le site internet de Rakuten, et plus particuliÃ¨rement les administrateurs de ce site.
Nous mettons en place une pipeline MLOps pour permettre l'automatisation, la scalabilitÃ©, et la fiabilitÃ© des modÃ¨les mis en place, et ce, dans le long terme.

Dans ce projet, nous entraÃ®nons un modÃ¨le multimodal utilisant deux modÃ¨les `VGG16` et `LSTM` dÃ©jÃ  existants et versionnÃ©s sur DasgHub. Il vous faudra donc les rÃ©cupÃ©rer.

Les mÃ©triques suivantes sont programmÃ©es pour mesurer la performance du modÃ¨le et du pipeline MLOps:
         5. Accuracy : Ã‰valuer la proportion de bonnes prÃ©dictions 
         6. F1-score : Prendre en compte le dÃ©sÃ©quilibre des classes 
         7. Temps dâ€™infÃ©rence : Mesurer la rapiditÃ© des prÃ©dictions du modÃ¨le

## DonnÃ©es et Infrastructure
1. **Sources des donnÃ©es** : stockÃ©s dans HuggingFace: https://huggingface.co/datasets/ndiguesene/ml-datasets-image-rakuten-ecommerce/resolve/main/
      - DonnÃ©es textuelles (~60 MB)
      - DonnÃ©es images (~2.5 GB)
   2. **Stockage** : fichiers plats (CSV)
   3. **Nettoyage & PrÃ©paration** : StratÃ©gie de gestion des donnÃ©es manquantes et transformation.
   4. **Infrastructure** : l'infrastructure de dÃ©ploiement mis en place se base entiÃ¨rement et exclusivement sur Apache Airflow et Docker.
3. **DÃ©veloppement du ModÃ¨le ML**
   1. **Choix des algorithmes** : Deep learning
   2. **Frameworks utilisÃ©s** : TensorFlow, Scikit-learn
4. **Industrialisation avec MLOps**
   1. **Gestion du versionnement des modÃ¨les** : MLflow
   2. **Versionnage des donnÃ©es, des mÃ©triques et artefacts** : DVC et DagsHub 
   3. **Orchestration** : Airflow
   4. **Monitoring & ObservabilitÃ©** : Prometheus, Grafana
5. **DÃ©ploiement et ScalabilitÃ©**
   1. **Mode de dÃ©ploiement** : Batch (REST API)
   2. **Infrastructure de dÃ©ploiement** : Docker et Airflow
   3. **Gestion du drift** : Non encore implÃ©mentÃ©
6. **SÃ©curitÃ© et Gouvernance**
   1. Routes d'API sÃ©curisÃ©es via l'usage du Json Web Token (JWT)


---------

# Vous Ãªtes prÃ©ssÃ©s ? DÃ©marrez rapidement ici
L'usage exclusif de Docker sur ce projet nous permet d'assurer la reproductibilitÃ©. Il est donc primordiale que vous ayez un Docker Engine up.
Il vous est, tout de mÃªme, conseillÃ© de crÃ©er un environnement virtuel Python lorsque vous souhaitez lancer ce projet.
CrÃ©ez l'environnement virtuel (exemple : `rakuten-project-mlops`) avec les Ã©tapes suivantes sur Linux :
 
```bash
python -m venv rakuten-project-mlops
```

Cela va crÃ©er un dossier `rakuten-project-mlops/` contenant lâ€™environnement virtuel.

Ensuite utilisez cet environnement pour toutes les commandes futures.

### Fichier .env
Le projet dispose d'un fichier `.env` non versionnÃ© contenant les informations nÃ©cessaires Ã  son bon fonctionnement. Vous pouvez vous inspirer du fichier `.env.example` pour crÃ©er votre fichier `.env`.
Les informations requises :

```
TOKENIZER_CONFIG_PATH=`RÃ©pertoire et nom du fichier artefact tokenizer format json`
LSTM_MODEL_PATH=`RÃ©pertoire et nom du modÃ¨le prÃ©-entraÃ®nÃ© lstm format h5` 
VGG16_MODEL_PATH=`RÃ©pertoire et nom du modÃ¨le prÃ©-entraÃ®nÃ© image VGG16 format h5`
BEST_WEIGHTS_PATH=`RÃ©pertoire et nom du fichier d'artefacts des poids prÃ©calculÃ©s format json` 
BEST_WEIGHTS_PATH_PKL=`RÃ©pertoire et nom du fichier d'artefacts des poids prÃ©calculÃ©s format pickle` 
MAPPER_PATH=`RÃ©pertoire et nom du fichier d'artefacts mapper sous format json` 
MAPPER_PATH_PKL=`RÃ©pertoire et nom du fichier d'artefacts mapper sous format pickle`
DATASET_PATH=`RÃ©pertoire d'enregistrement des donnÃ©es textuelles d'entraÃ®nement de test, doit Ãªtre dans $DATA_PATH`
DATA_PATH=`RÃ©pertoire d'enregistrement des donnÃ©es d'entraÃ®nement`
IMAGES_PATH=`RÃ©pertoire d'enregistrement des images d'entraÃ®nement de test, doit Ãªtre dans $DATA_PATH`
PREDICTIONS_PATH=`RÃ©pertoire d'enregistrement des prÃ©dictions test effectuÃ©s aprÃ¨s entrainement`
CONCATENATED_MODEL_PATH=`RÃ©pertoire d'enregistrement du modÃ¨le concatÃ©nÃ©`
SERVING_LOGGER_PATH=`RÃ©pertoire et nom du fichier de logs de l'API dans le conteneur`
SECRET_KEY=`ClÃ© secrete utilisÃ©e pour le hachage`
ALGORITHM=`Algorithme de hachage pour gÃ©nÃ©ration toke JWT, dÃ©faut HS256`
ACCESS_TOKEN_EXPIRE_MINUTES=`DÃ©lai d'expiration du token JWT, dÃ©faut 30 minutes`
TEST_MODEL_LOGGER_PATH=`RÃ©pertoire et nom du fichier de logs de test du modÃ¨le dans le conteneur`
TRAIN_MODEL_LOGGER_PATH=`RÃ©pertoire et nom du fichier de logs d'entraÃ®nement dans le conteneur`
IMPORT_DATA_LOGGER_PATH=`RÃ©pertoire et nom du fichier de logs dans le conteneur`
AUTH_SERVICE_LOGGER_PATH=`RÃ©pertoire et nom du fichier de logs dans le conteneur`
MLFLOW_TRACKING_URI=https://dagshub.com/MariamaNadia/Projet_Formation_MLOps_DataScientest_2025.mlflow
MLFLOW_EXPERIMENT_NAME=ProductCodeClassifier
DAGSHUB_USERNAME=`Votre identifiant DagsHub`
DAGSHUB_TOKEN=`Votre token DagsHub`
AIRFLOW_UID=0
PROJECT_HOST_PATH=`RÃ©pertoire absolu de votre projet, requis pour l'usage de l'opÃ©rateur Airflow DockerOperator`
TEST_USER=`Login de l'utilisateur de l'API`
TEST_USER_FULLNAME=`Nom de l'utilisateur de l'API`
TEST_USER_PASSWORD=`Mot de passe de l'utilisateur de l'API`
```

### DÃ©pendances
Chaque module de ce projet contient ses dÃ©pendances dans un fichier requirements qui lui est propre. Ce projet utilise Git pour gÃ©rer le versionnement du code. Il est donc constituÃ© de plusieurs branches. Les donnÃ©es requises pour le dÃ©veloppement du modÃ¨le (si vous entraÃ®nez le modÃ¨le from scratch)  sont versionnÃ©es via DVC DagsHub avec option d'enregistrement sur S3. Les Ã©tapes suivantes vous montrent comment configurer ces outils. Les Ã©tapes suivantes sont effectuÃ©es sur le rÃ©pertoire racine du projet.

### Configurer un stockage distant avec DVC
Nous utilisons **S3 dâ€™AWS** pour sauvegarder les donnÃ©es. Pour cela, il faut d'abord installer S3 Ã  lâ€™aide de la commande suivante :
 
```bash
pip install "dvc[S3]"
```

**Initialiser DVC dans le projet**

```bash
  dvc init
```

**Configurer DagsHub comme stockage distant** 
Une fois le support de S3 installÃ©, il faudra mettre Ã  jour le fichier de configuration `.dvc/config` pour ajouter DagsHub comme votre stockage distant. Les commandes Ã  suivre sont les suivantes : 

```bash
dvc remote add origin s3://dvc
dvc remote modify origin endpointurl https://dagshub.com/MariamaNadia/Projet_Formation_MLOps_DataScientest_2025.s3

```
Ici, `votre_token(*)` est Ã  rÃ©cupÃ©rer sur l'interface de DagsHub. Il sera par ailleurs utilisÃ© dans votre pipeline Airflow pour pousser les donnÃ©es automatiquement vers DVC.

```bash
dvc remote modify origin --local access_key_id votre_koken(*)
dvc remote modify origin --local secret_access_key votre_token(*)
```

Votre fichier `.dvc/config`(crÃ©Ã© sur initialisation de dvc sur le rÃ©pertoire du projet)  devrait ressembler Ã  ceci : 

```bash
[core]
    remote = origin
['remote "origin"']
    url = s3://dvc
    endpointurl = https://dagshub.com/MariamaNadia/Projet_Formation_MLOps_DataScientest_2025.s3
```

### RÃ©cupÃ©rer les artefacts et modÃ¨les et donnÃ©es dÃ©jÃ  existants
`Attention`, cette Ã©tape est trÃ¨s importante. Sans elle, vous ne pouvez avoir les artefacts et les modÃ¨les pour poursuivre le traitement.
Dans ce projet, nous entraÃ®nons un modÃ¨le multimodal utilisant deux modÃ¨les VGG16 et LSTM dÃ©jÃ  existants et versionnÃ©s sur DasgHub. Il vous faudra donc les rÃ©cupÃ©rer.
Vous pouvez rÃ©cupÃ©ree les donnÃ©es lorsque vous souhaitez rÃ©entraÃ®ner le modÃ¨le.

```bash
dvc pull
```

### Lancer la pipeline Airflow
Pour un premier lancement, les bases de Airflow seront crÃ©Ã©es aisni que leurs volumes associÃ©s, les rÃ©pertoires nÃ©cessaires Ã  Aifrlow en local sont Ã©galement crÃ©Ã©s.

```bash
make init-airflow
```

Pour lancer la pipeline, utilisez la commande suivante :

```bash
make start
```

Pour arrÃªter la pipeline, utilisez la commande suivante :

```bash
make stop
```

### Comment tester les endpoints
Un utilisateur a Ã©tÃ© crÃ©Ã© par dÃ©faut pour pouvoir tester. Vous pouvez configurer ses accÃ¨s dans le fichier `.env` Ã  travers les paramÃ¨tres (`TEST_USER,TEST_USER_FULLNAME,TEST_USER_PASSWORD`).

---------
## Structure du projet

### Structure globale
```
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ REAMDME.md
â”œâ”€â”€ __init__.py
â”œâ”€â”€ airflow
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ config
â”‚   â”‚   â””â”€â”€ airflow.cfg
â”‚   â”œâ”€â”€ dags
â”‚   â”‚   â””â”€â”€ ml_pipeline.py
â”‚   â”œâ”€â”€ initialize_airflow.sh
â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”œâ”€â”€ dag_id=ml_pipeline_dvc
â”‚   â”‚   â””â”€â”€ dag_processor
â”‚   â”œâ”€â”€ plugins
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ predictions
â”‚   â”‚   â”œâ”€â”€ predictions.json
â”‚   â”‚   â””â”€â”€ predictions.json.dvc
â”‚   â””â”€â”€ raw
â”‚       â”œâ”€â”€ X_test_update.csv
â”‚       â”œâ”€â”€ X_test_update.csv.dvc
â”‚       â”œâ”€â”€ X_train_update.csv
â”‚       â”œâ”€â”€ X_train_update.csv.dvc
â”‚       â”œâ”€â”€ Y_train_CVw08PX.csv
â”‚       â”œâ”€â”€ Y_train_CVw08PX.csv.dvc
â”‚       â”œâ”€â”€ __MACOSX
â”‚       â”œâ”€â”€ image_test
â”‚       â”œâ”€â”€ image_train
â”‚       â””â”€â”€ images_low.zip
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ image.png
â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ api.log
â”‚   â”œâ”€â”€ auth_service_logger.log
â”‚   â”œâ”€â”€ import_data_logger.log
â”‚   â”œâ”€â”€ lstm_accuracy_curves.png
â”‚   â”œâ”€â”€ lstm_loss_curves.png
â”‚   â”œâ”€â”€ serving_logger.log
â”‚   â”œâ”€â”€ test_model_logger.log
â”‚   â”œâ”€â”€ train
â”‚   â”œâ”€â”€ train_model_logger.log
â”‚   â”œâ”€â”€ validation
â”‚   â”œâ”€â”€ vgg16_accuracy_curves.png
â”‚   â””â”€â”€ vgg16_loss_curves.png
â”œâ”€â”€ mlartifacts
â”œâ”€â”€ mlruns
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ best_lstm_model.h5
â”‚   â”œâ”€â”€ best_lstm_model.h5.dvc
â”‚   â”œâ”€â”€ best_vgg16_model.h5
â”‚   â”œâ”€â”€ best_vgg16_model.h5.dvc
â”‚   â”œâ”€â”€ best_weights.json
â”‚   â”œâ”€â”€ best_weights.json.dvc
â”‚   â”œâ”€â”€ best_weights.pkl
â”‚   â”œâ”€â”€ best_weights.pkl.dvc
â”‚   â”œâ”€â”€ concatenate.h5
â”‚   â”œâ”€â”€ concatenate.h5.dvc
â”‚   â”œâ”€â”€ mapper.json
â”‚   â”œâ”€â”€ mapper.json.dvc
â”‚   â”œâ”€â”€ mapper.pkl
â”‚   â”œâ”€â”€ mapper.pkl.dvc
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â””â”€â”€ tokenizer_config.json.dvc
â”œâ”€â”€ monitoring
â”‚   â”œâ”€â”€ grafana
â”‚   â”‚   â”œâ”€â”€ Product_Classifier_Monitoring_1756323298849.json
â”‚   â”‚   â”œâ”€â”€ fastapi_metrics.json
â”‚   â”‚   â””â”€â”€ provisioning
â”‚   â”œâ”€â”€ node_exporter-1.8.1.linux-amd64
â”‚   â”‚   â”œâ”€â”€ LICENSE
â”‚   â”‚   â”œâ”€â”€ NOTICE
â”‚   â”‚   â””â”€â”€ node_exporter
â”‚   â”œâ”€â”€ node_exporter-1.8.1.linux-amd64.tar.gz
â”‚   â””â”€â”€ prometheus
â”‚       â””â”€â”€ prometheus.yml
â”œâ”€â”€ notebooks
â”‚   â””â”€â”€ Rakuten.ipynb
â”œâ”€â”€ reports
â”‚   â”œâ”€â”€ detailled_pipeline.png
â”‚   â”œâ”€â”€ endpoint_up_targets.PNG
â”‚   â”œâ”€â”€ grafana_prometheus_data_source.PNG
â”‚   â”œâ”€â”€ metriques_graph_prometheus.PNG
â”‚   â”œâ”€â”€ mlflow_metrics.png
â”‚   â”œâ”€â”€ mlfow_experiment.png
â”‚   â”œâ”€â”€ pipeline.png
â”‚   â”œâ”€â”€ predict_endpoint_input.png
â”‚   â”œâ”€â”€ predict_endpoint_return.png
â”‚   â””â”€â”€ securing_api_authorized.png
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_compose_options.sh
â”œâ”€â”€ setup.py
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ auth_service
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ security_logic.py
â”‚   â”œâ”€â”€ config
â”‚   â”œâ”€â”€ data
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ build_data
â”‚   â”œâ”€â”€ features
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ build_features.py
â”‚   â””â”€â”€ models
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ predict
â”‚       â”œâ”€â”€ serve
â”‚       â””â”€â”€ train
â””â”€â”€ tests
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_api.py
```

### Struture du code des stages : data, train, auth, predict

```
.
â”œâ”€â”€ __init__.py
â”œâ”€â”€ auth_service
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ security_logic.py
â”œâ”€â”€ config
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ build_data
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ check_structure.py
â”‚       â”œâ”€â”€ import_raw_data.py
â”‚       â”œâ”€â”€ make_dataset.py
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ up.sh
â”œâ”€â”€ features
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ build_features.py
â””â”€â”€ models
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ predict
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ predict.py
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â””â”€â”€ up.sh
    â”œâ”€â”€ serve
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ auth_utils.py
    â”‚   â”œâ”€â”€ predict_logic.py
    â”‚   â”œâ”€â”€ requirements.txt
    â”‚   â”œâ”€â”€ serve_model_fastapi.py
    â”‚   â”œâ”€â”€ serve_model_fastapi_old.py
    â”‚   â””â”€â”€ up.sh
    â””â”€â”€ train
        â”œâ”€â”€ Dockerfile
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ main.py
        â”œâ”€â”€ requirements.txt
        â”œâ”€â”€ train_model.py
        â””â”€â”€ up.sh
```


---------
## DÃ©tails d'implÃ©mentation

## Serving, dockerisation et tests unitaires
### Restructuration des rÃ©pertoires et fichiers

#### data : build
Ici, nous regroupons les logiques de rÃ©cupÃ©ration des donnÃ©es depuis le dÃ©pÃ´t distant. Les donnÃ©es structurÃ©es sont rÃ©cupÃ©rÃ©es depuis AWS DataScientest. Les donnÃ©es d'images sont rÃ©cupÃ©rÃ©es depuis HiggingFace.

#### train
Ici, nous entraÃ®nons le modÃ¨le (contient les fichiers main.py et train_model.py). Le rÃ©pertoire contiendra un Dockerfile spÃ©cifique pour lancer l'entraÃ®nement du modÃ¨le Ã  la demande. Les rÃ©sultats seront enregistrÃ©s dans `models` Ã  la racine

#### predict 
Ici, on applique le modÃ¨le au donnÃ©es d'entraÃ®nement (contient le fichier predict.py). Le rÃ©pertoire contiendra un Dockerfile spÃ©cifique pour lancer le test du modÃ¨le sur les donnÃ©es d'entraÃ®nement; Les rÃ©sultats seront enregistrÃ©s sur data/predictions Ã  la racine

#### serve
Ici, on utilisera dans un premier temps requests ou FastAPI pour crÃ©er une API qui va interroger le modÃ¨le avec des donnÃ©es prÃ©alablement renseignÃ©es (fichier serve.py Ã  crÃ©er) et retournera des rÃ©sultats que l'on pourra mettre dans data/predictions Ã  la racine avec un nom de fichier spÃ©cifique
Niveaux de serving : 
1. Usage de FastAPI pour crÃ©er un contneur basique de serving
2. Usage de BentoML pour automatiser le dÃ©ploiement, serving etc

#### Variables d'environnement : vous devez rajouter un fichier .env sur la racine du projet
Les chemins des modÃ¨les (poids, mapper etc) ne sont plus codÃ©s en dur. Les modifications suivantes ont Ã©tÃ© apportÃ©es : 
1. ajout fichier `.env` contenant les variables d'environnement (**ce fichier ne doit pas Ãªtre poussÃ© sur le git, il peut renfermer des informations sensibles tels clÃ©s d'api etc**)
2. ajout fichier `.env.example` sur le git : fichier d'exemple montrant comment renseigner le fichier .env ( quelles sont les variables d'environnement etc)

## Conteneurisation
Docker est utilisÃ© pour conteneuriser les diffÃ©rents services de l'application. Chaque service dispose d'un rÃ©pertoire spÃ©cifique sur les sources ainsi qu'un Dockerfile spÃ©cifique.

### Variables d'environnement
Le fichier `.env` Ã  la racine du projet, doit Ãªtre mis en jour avec les rÃ©pertoires de volume crÃ©Ã©s dans le conteneur (Exemple : `TOKENIZER_CONFIG_PATH=/app/models/tokenizer_config.json`).

### Services applicatifs
1. Service de donnÃ©es
Ici, le service effectue le tÃ©lÃ©chargement des donnÃ©es depuis le dÃ©pÃ´t HuggingFace : `https://huggingface.co/datasets/ndiguesene/ml-datasets-image-rakuten-ecommerce/resolve/main/`.

`Package` : src/data/build_data/

2. Service d'entraÃ®nement

`Package` : src/models/train
Contient les fichiers : 
- .dockerignore : fichier prÃ©cisant les fichiers et rÃ©pertoires Ã  ignorer lors de la construction de l'image
- Dockerfile : commandes pour crÃ©er l'image Docker pour ce service uniquement
- main.py : script principal contenant la logique d'entrainement global 
- train_model.py  : script contenant la logique d'entrainement des modÃ¨les
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les dÃ©pendences requis pour crÃ©er l'image 

3. Service de test

Ici, nous utilisons les donnÃ©es d'entraÃ®nement pour tester l'usage du modÃ¨le.
`Package` : src/models/predict
Contient les fichiers : 
- .dockerignore : fichier prÃ©cisant les fichiers et rÃ©pertoires Ã  ignorer lors de la construction de l'image
- Dockerfile : commandes pour crÃ©er l'image Docker pour ce service uniquement
- predic.py : script principal contenant la logique de test du modÃ¨le en utilisant le modÃ¨le entraÃ®nÃ© et sauvegardÃ© sur le service d'entraÃ®nement
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les dÃ©pendences requis pour crÃ©er l'image 

4. Service de serving

Ici, nous crÃ©ons une application permettant d'intÃ©rroger le modÃ¨le via API en utilisant FastAPI.
`Package` : src/models/serve
Contient les fichiers : 
- .dockerignore : fichier prÃ©cisant les fichiers et rÃ©pertoires Ã  ignorer lors de la construction de l'image
- Dockerfile : commandes pour crÃ©er l'image Docker pour ce service uniquement
- predic_logic.py : wrapper crÃ©Ã© pour contenir les fonctions requis pour effectuer les prÃ©dictions (traitement de l'image, traitement des donnÃ©es textuelles)
- serve_model_fastapi.py : script principal contenant l'application FastAPI (les endpoints de prÃ©diction)
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les dÃ©pendences requis pour crÃ©er l'image

### API de prÃ©diction
Le service de prÃ©diction est composÃ© pour le moment de deux endpoints : 
- `/status` : pour obtenir le statut de l'application. Retourne un message si l'application est up.
- `/predict` :  pour obtenir une classifiction d'un code produit en fonction des informations suivantes : </br>

`product_identifier`: str = Form(...), # Un identifiant entier pour le produit. Cet identifiant est utilisÃ© pour associer le produit Ã  son code de type de produit correspondant. </br>
`designation`: str = Form(...), # Le titre du produit, un court texte rÃ©sumant le produit </br>
`description`: str = Form(...),  # Un texte plus dÃ©taillÃ© dÃ©crivant le produit. Tous les commerÃ§ants n'utilisent pas ce champ, donc, afin de conserver lâ€™originalitÃ© des donnÃ©es, le champ description peut contenir des valeurs NaN pour de nombreux produits. </br>
`product_id`: str = Form (...), # Un identifiant unique pour ce produit </br>
`imageid`: str = Form (...), # Un identifinat unique de l'image associÃ© Ã  ce produit. </br>
`image`: UploadFile = File(...)  # L'image correspondant au produit </br>

![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_input.png)
![Retour /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_return.png)

### Services : sÃ©curisation et logging

##### DÃ©couplage en services
Chaque fonctionnalitÃ© du systÃ¨me (tÃ©lÃ©chargement et traitement des donnÃ©es, entraÃ®nement du modÃ¨le concatÃ©nÃ©, test du modÃ¨le entraÃ®nÃ© et serving) est dÃ©fini en service respectivement situÃ©s dans les rÃ©pertoires : src/data, src/models/train, src/models/predict, src/models/serve.
Chaque service est utilisable de faÃ§on dÃ©couplÃ© via des conteneurs Docker ainsi que tous les packages requis pour son bon fonctionnement.

##### Limitation taux de requÃªte
La limitation du dÃ©bit permet d'Ã©viter les abus et de garantir une utilisation Ã©quitable de l'API.
La limite de 10 requÃªtes par minutes sur /predict est appliquÃ©e.
Un gestionnaire d'exception global pour RateLimitExceeded garantit que les utilisateurs reÃ§oivent un message d'erreur clair lorsque les limites sont dÃ©passÃ©es.

##### Authentification et authorisation
Nous avons mis en Å“uvre OAuth2 avec JWT pour sÃ©curiser les points de terminaison. Les clÃ©s d'authenication sont tous sauvegardÃ©s comme variables d'environnement.
Le point de terminaison `/token` gÃ©nÃ¨re des jetons d'accÃ¨s, et la dÃ©pendance `get_current_user` garantit que seuls les utilisateurs authentifiÃ©s peuvent accÃ©der aux points de terminaison protÃ©gÃ©s comme `/predict`.
La protection des points de terminaison sensibles comme `/predict` garantit que seuls les utilisateurs autorisÃ©s peuvent accÃ©der aux modÃ¨les. 
La logique de sÃ©curitÃ© est sÃ©parÃ©e dans security_logic.py, ce qui rend la base de code plus modulaire et rÃ©utilisable.
Dans cette version, les utilisateurs ne sont pas dans une base de donnÃ©es.
- `security_logic.py` : wrapper contenant les logiques d'authentification et d'authorisation utilisÃ©s par le script principal.

##### Logging
Nous avons mis en place un intergiciel qui enregistre chaque demande avec un identifiant unique, une mÃ©thode, une URL, un code d'Ã©tat et un temps de traitement.
La journalisation est essentielle pour le dÃ©bogage, la surveillance et l'audit.
L'inclusion des identifiants des requÃªtes et des temps de traitement facilite le suivi et l'optimisation des performances. 

- `import_data_logger.log`
- `train_model_logger.log`
- `test_model_logger.log`
- `serving_logger.log`

Nous avons inclu un (RotateFileHandler) pour gÃ©rer la limitation de la taille des fichiers de logs.

##### Chargement des modÃ¨les au dÃ©marrage
Nous avons rajoutÃ© une mÃ©thode au dÃ©marrage de l'application de prÃ©diction pour charger tous les modÃ¨les et fichiers de configuration pour Ã©viter leur chargement Ã  chaque demande de prÃ©diction.

##### Orchestration
Le fichier `docker-compose.xml` Ã  la racine du projet est utilisÃ© pour orchestrer tous les services du projet. Nous utilisons le fichier `.env` pour assurer la rÃ©usabilitÃ© des diffÃ©rents environnemenst (production, staging etc) et la modularitÃ©.

##### Technologies utilisÃ©es
1. Docker : utilisÃ© pour crÃ©er chaque service de l'application en un conteneur indÃ©pendant
2. Logging et FileHandler : nous utilisons un logging centralisÃ© pour chaque service (tÃ©lÃ©chargement et traitement des donnÃ©es, entraÃ®nement du modÃ¨le concatÃ©nÃ©, test du modÃ¨le entraÃ®nÃ© et serving).

Chaque service retourne un fichier log spÃ©cifique disponible dans le rÃ©pertoire logs/ Ã  la racine du projet.
3. Slowapi : utilisÃ© pour limiter le taux de requÃªte sur les routes.
4. Oauth2 et JWT : sÃ©curisation des routes d'API en terme d'authentification et d'authorisation.
5. bcrypt : encryption des informations utilisateurs (mots de passe) pour comparaison et validation des informations

##### Usage : 

- Demander un token : `/token`
`curl -X POST "http://127.0.0.1:8000/token" -d "username=user1&password=43SoYourAreADataEngineer34"`
Exemple retours : 
{
    "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
    "token_type": "bearer"
}
- Utiliser le token pour intÃ©rroger l'API de prÃ©diction : `/predict_code`

![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/securing_apis/reports/securing_api_authorized.png)

---------

## Tests Unitaires
## Documentation des tests unitaires

Cette partie de la documentation dÃ©crit les tests unitaires dÃ©finis pour lâ€™API FastAPI. Ils couvrent les trois endpoints principaux : racine (`/`), `GET /status` et `POST /predict`.

---

## PrÃ©requis

- Python 3.9
- FastAPI
- Uvicorn (pour lancer lâ€™API si besoin)
- pytest
- httpx (installÃ© automatiquement avec FastAPI)

Installer les dÃ©pendances :

```bash
  pip install fastapi uvicorn pytest httpx
```

## Lancer les tests

Depuis la racine du projet (lÃ  oÃ¹ se trouve src/ et tests/), exÃ©cuter :

```bash
   pytest tests/test_api.py
```
## Structure des tests

Le fichier tests/test_api.py contient trois fonctions :
```python
from fastapi.testclient import TestClient
from Projet_Formation_MLOps_DataScientest_2025.api import app

client = TestClient(app)
```
Chaque test utilise un TestClient pour simuler des requÃªtes HTTP vers lâ€™application FastAPI.

1. **test_status**
   2. Objectif : vÃ©rifier que lâ€™endpoint GET /status renvoie bien un code HTTP 200 et le message attendu. 
   3. script
      ```python
      def test_status():
          response = client.get("/status")
          assert response.status_code == 200
          assert response.json() == {"status": "L'API fonctionne correctement"}
      ```
   3. RequÃªte : 
   ```http request
   GET /status
   ```
   4. Assertions :
      - `response.status_code == 200`
      - `response.json() == {"status": "L'API fonctionne correctement"}`

   2. **test_root**
      3. Objectif : valider que la racine / renvoie un message de bienvenue.
      4. script
      ```python
      def test_root():
          response = client.get("/")
          assert response.status_code == 200
          data = response.json()
          assert "message" in data
          assert "Bienvenue" in data["message"]
         ```
      4. RequÃªte :
      ```http request
      GET /
      ```
      5. Assertions
         - `response.status_code == 200`
         - `Le corps JSON contient une clÃ© message`
         - `La chaÃ®ne "Bienvenue" apparaÃ®t dans la valeur de message`

3. **test_predict**
   4. Objectif : sâ€™assurer que lâ€™endpoint POST /predict accepte un payload JSON arbitraire et retourne un JSON avec les clÃ©s predicted et label.
   5. script
   ```python
   def test_predict():
       sample_text = {"predicted": "1", "label": "label_1"}
       response = client.post("/predict", json=sample_text)
       assert response.status_code == 200
       data = response.json()
       assert "predicted" in data
       assert "label" in data
   ```
   5. RequÃªte :
   ```http request
   POST /predict
   Content-Type: application/json
   {"predicted": "1", "label": "label_1"}
   ```
   6. Assertions :
    - response.status_code == 200 
    - La rÃ©ponse JSON contient les champs predicted et label

---------

## Automatisation Airflow/DVC/DagsHub/MLFlow

A partir de lÃ , nous travaillons exclusvement avec Docker.
Donc, il faut obligatoirement disposer d'un fichier `.env` Ã  la racine du projet. Exemple de contenu `.env` disponible sur le fichier `.env.example``.

1. AprÃ¨s avoir cloner le code, crÃ©er un fichier `.env` (ne pas oublier de mettre Ã  jour vos informations dagshub (token))
```bash
touch .env

```

2. DonnÃ©es nÃ©cessaires : fichiers artefacts non disponibles sur Git
Les fichiers .h5, .json, .pkl (best_lstm_model, best_vgg16, best_weights, mapper, tokenizer) doivent Ãªtre disponibles dans le dossier `models`.
Ils ont utilisÃ© dans l'entraÃ®nement des modÃ¨les.
A la rÃ©cupÃ©ration du projet, si ces fichiers ne sont pas disponibles, faire une commande : 

```bash
dvc pull
```

3. ExÃ©cuter la pipeline sans Airflow (tests uniquement)

```bash
docker compose up

```

!! Le fichier Ã  la racine `run_compose_options.sh` contient des examples de commandes pour exÃ©cuter chaque service en particulier(Tests unqiuement).

4. ExÃ©cuter la pipeline via Airflow en mode standalone

Lancer une instance airflow en local (juste pour des tests) : mettre Ã  jour le fichier sous airflow/dags/initialize_airflow.sh et exÃ©cuter le fichier

```bash
export AIRFLOW_HOME=`rÃ©pertoire absolu du projet`/airflow  
airflow db migrate

# DÃ©marrer airflow en mode standalone (crÃ©e un utilisateur admin automatiquement, les credentials sont enregsitrÃ©s sous : airflow/simple_auth_manager_passwords.json.generated )
airflow standalone

```

5. ExÃ©cuter la pipeline via Airflow et Docker
Dans cette partie du projet, nous entrons dans l'industrialisation.  Voir le point suivant.

---------

### Architecture MLOps
Pour automatiser l'exÃ©cution des tÃ¢ches end-to-end, nous mettons en contribution Airflow(gestion de l'ordonnancement et exÃ©cution automatique des tÃ¢ches), Docker(encapsulation des logiques avec toutes les dÃ©pendances) et DVC(gestionnaire du versionnage des donnÃ©es).

#### Motivations
Dans une pipeline de MLOps comme celui qui nous concerne, il est bien de s'assurer de certains aspects : 
- les donnÃ©es utilisÃ©es Ã  chaque exÃ©cution du modÃ¨le sont connus et rÃ©cupÃ©rables, ceci rejoint l'aspect reproductibilitÃ©
- la version entraÃ®nÃ©e du modÃ¨le est connu, suavegardÃ© et rÃ©cupÃ©rable, toujours sur l'aspect reproductibilitÃ©
- les artefacts du modÃ¨le correspondant Ã  cette version d'exÃ©cution sont enregistrÃ©s et rÃ©cupÃ©rables
- la version gÃ©nÃ©rale du code est enregistrÃ©e et rÃ©cupÃ©rable
- les exÃ©cutions sont agnostiques de la plateforme utilisÃ©e

#### Outils

- Airlfow nous permet de gÃ©rer l'ordonnancement et l'exÃ©cution automatique des diffÃ©rentes tÃ¢ches requises pour entraÃ®ner le modÃ¨le
- MLfow nous permet de sauvegarder le modÃ¨le, les mÃ©triques ainsi que les artefacts produits aprÃ¨s chaque entraÃ®nement
- DVC nous permet d'assurer la sauvegarde et le versionning des donnÃ©es utilisÃ©es lors de l'entraÃ®nement, le modÃ¨le produit, les fichiers artefacts produits
- Docker nous permet d'assurer l'encapsulation de touts les prÃ©requis Ã  l'exÃ©cution de tous les stages de notre pipeline d'entraÃ®nement
- Git nous permet d'assurer la sauvegarde de tous les fichiers de code requis pour la bonne exÃ©cution et la crÃ©ation des images et conteneurs Docker

#### Configurations

##### Fichier .env
Le fichier .env est un fichier trÃ¨s important pour le projet se trouvant sur la racine. Il contient toutes les variables de configuration nÃ©cessaires pour la bonne exÃ©cution de la pipeline MLFlow.
Il contient les liens vers les fichiers requis Ã  l'entraÃ®nement du modÃ¨le (poids, tokenizer etc). Il contient Ã©galement, les rÃ©pertoires d'enregitrement des donnÃ©es ainsi que du modÃ¨le.
Ces derniers sont utilisÃ©s notamment par DVC pour le versionning des donnÃ©es.
Il contient Ã©galement les secret pour le module d'authentification.
Ce fichier n'est pas versionnÃ© mais un fichier exemple (`.enf.example`) est disponible sur le rÃ©pertoire distant.

##### DVC et MLflow
Nous utilisons DVC via DagsHub S3 en plus du tracking distant MLflow liÃ© Ã  DagsHub.
Les commandes de configuration DVC Ã  appliquer en local est disponible sur l'interface DagsHub.

Les configurations suivantes doivent Ãªtre renseignÃ©es sur le fichier .env
`MLFLOW_TRACKING_URI` : correspond au lien distant MLflow liÃ© Ã  DagsHub.
`MLFLOW_EXPERIMENT_NAME` : par ProductCodeClassifier
`DAGSHUB_USERNAME`: Nom d'utilisateur de votre compte DagsHub crÃ©Ã©
`DAGSHUB_TOKEN` : Votre token DagsHub

Ici, nous dÃ©marrons la connexion Ã  MLflow distant (src/models/train/main.py) :

```python
with mlflow.start_run(run_name="Train_Concatenate_Model") as run:
  mlflow.set_tag("source", "airflow")
  mlflow.set_tag("version", run_time)
  mlflow.set_tag("model_type", "VGG16+LSTM")
```

AprÃ¨s entraÃ®nement, les artefacts et le modÃ¨le sont sauvegardÃ©s :
```python
 # Log artefacts
  mlflow.log_artifact(tokenizer_config_path)
  mlflow.log_artifact(mapper_path_pkl)
  mlflow.log_artifact( best_weights_path_pkl)
  mlflow.log_artifact(BEST_WEIGHTS_PATH)
  mlflow.log_artifact(MAPPER_PATH)

  # Log the models
  mlflow.keras.log_model(lstm, "lstm_model")
  mlflow.keras.log_model(vgg16, "vgg16_model")
  mlflow.keras.log_model(concatenate_model, "concatenate_model")
```

##### Docker
Comme prÃ©sentÃ© plus haut, les diffÃ©rents stages de notre pipeline sont dockerisÃ©s. Chaque stage dispose d'un Dockerfile permettant de crÃ©er automatiquement une image.

##### Airflow
Nous utilisons Ariflow sous Docker, nous rÃ©cupÃ©rons donc le docker-compose officiel dÃ©diÃ© sur le site de Airflow (https://airflow.apache.org/docs/apache-airflow/3.0.4/docker-compose.yaml).
Ce fichier a Ã©tÃ© mis Ã  jour et est disponible Ã  la racine du projet : `docker-compose.yml`.
Il a Ã©tÃ© mis Ã  jour suivant : 
- les volumes : nous lions le projet en local aux diffÃ©rents conteneurs dÃ©marrÃ©s par Ariflow pour permettre la disponbilitÃ© des logiques et fichiers de code (` - ./:/opt/airflow/mlops-project `) nÃ©cessaires Ã  la bonne exÃ©cution des stages en particulier des logiques pour crÃ©er les images des stages (Dockerfile).
- l'utilisateur admin airflow

L'exÃ©cution de  ce fichier produit les conteneurs/services suivants :  
- la base de  donnÃ©es PostgreSQL
- la base de donnÃ©es Redis
- le serveur d'api de Airflow
- le scheduler de Airflow
- le dag processor de Ariflow
- un worker
- un Triggerer

#### Stages de la pipeline MLflow

- DÃ©finition 
La dÃ©finition de la pipeline sous forme de dag Airflow est disponible sous `airflow/dags/ml_pipeline.py`.
Les opÃ©rateurs Docker : `BashOperator` et `DockerOperator` ont Ã©tÃ© utilisÃ©s.

Les diffÃ©rents stages de notre pipeline ont Ã©tÃ© regroupÃ©s et se prÃ©sentent comme suit : 

![Pipeline global Airflow](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/automating_dvc_mlflow_docker/reports/pipeline.png)

1. RÃ©cupÃ©ration des donnÃ©es : 3 opÃ©rateurs Airflow
- construction de l'image Docker contenant la logique de rÃ©cupÃ©ration des donnÃ©es
- crÃ©ation et lancement du conteneur de rÃ©cupÃ©ration des donnÃ©es
- enregistrement des donnÃ©es rÃ©cupÃ©rÃ©es sur DVC

2. EntraÃ®nement du modÃ¨le : 3 opÃ©rateurs Airflow
- crÃ©ation de l'image Docker contenant la logique d'entraÃ®nement du modÃ¨le
- crÃ©ation et lancement du conteneur d'entraÃ®nement du modÃ¨le - enregistrement du modÃ¨le et des artÃ©facts sur MLflow
- enregistrement du modÃ¨le et des fichiers sur DVC

3. Test du modÃ¨le : 3 opÃ©rateurs Airflow
- crÃ©ation de l'image de test
- crÃ©ation et lancement du conteneur de test
- enregistrement des prÃ©dictions rÃ©sultats sur DVC

4. Service d'exposition de l'API : 4 opÃ©rateurs Airflow
- crÃ©ation image pour l'authentification
- crÃ©ation et lancement du conteneur d'authentification : ce conteneur s'exÃ©cute en backgroud aprÃ¨s lancement
- crÃ©ation de l'image d'exposition de l'API
- crÃ©ation et lancement du conteneur d'exposition de l'API 

#### Lancement
- Premier lancement : initialise les bases Postgres SQL et Redis :
```bash
make init-airflow
```

- Lancement : 
```bash
make start
```

![Pipeline global Airflow](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/automating_dvc_mlflow_docker/reports/detailled_pipeline.png)

#### Ports ouverts requis par la pipeline

Le service d'authentification est ouvert sur le port 8011:8011.
Le service d'API est ouvert sur le port 8000:8000.

# Monitoring

IntÃ¨grer un systÃ¨me de monitoring complet basÃ© sur **Prometheus** et **Node Exporter**, avec exposition des mÃ©triques de l'application **FastAPI**.

## 1.  Installation des dÃ©pendances
Les services de monitoring sont intÃ©grÃ©s Ã©galement dans notre docker compose.

```yaml
...
  prometheus:
    image: prom/prometheus
    container_name: prometheus
    networks:
      - product_classier
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
  
  grafana:
    image: grafana/grafana-enterprise
    container_name: grafana
    networks:
      - product_classier
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
...
```

## 2.  Mise Ã  jour du code FastAPI

Ajout de lâ€™instrumentation Prometheus dans le fichier `src.models.serve.serve_model_fastapi.py` :

```python
from prometheus_fastapi_instrumentator import Instrumentator

# Ajout aprÃ¨s la crÃ©ation de l'application FastAPI
app = FastAPI()

# Instrumentation Prometheus
Instrumentator().instrument(app).expose(app)

```
## 3. Configuration prometheus.yml
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  - job_name: "fastapi"
    static_configs:
      - targets: ["serving_service_container:8000"]

  - job_name: "node"
    static_configs:
      - targets: ["localhost:9100"]
```

## 4.  VÃ©rification de lâ€™endpoint /metrics , prometheus, node exporter

Ouvrir dans le navigateur ou via curl :

```bash
http://localhost:8000/metrics

```


```bash
//http://localhost:9090

```


```bash
http://http://localhost:9100
```

## 6. MÃ©triques collectÃ©es (exemple) :
- `http_requests_total{job="fastapi"}` â€” nombre de requÃªtes HTTP par endpoint, mÃ©thode, code
- `process_cpu_seconds_total` â€” temps CPU utilisÃ©
- `node_memory_Active_bytes` â€” RAM active

### â†’ Installer et configurer Grafana, le connecter Ã  Prometheus et visualiser les mÃ©triques (FastAPI, Node Exporter) avec des dashboards.
Ã‰tape 1 : Lancer Grafana avec Docker

``` bash
docker run -d \
  -p 3000:3000 \
  --name=grafana \
  grafana/grafana

```
Demarrer grafana si dÃ©ja crÃ©e
``` bash
docker start grafana
docker restart grafana

```
Ã‰tape 2 : Ajouter Prometheus comme source de donnÃ©es
AccÃ¨dez Ã  Grafana : http://localhost:3000
Menu latÃ©ral gauche â†’ âš™ï¸ Configuration â†’ Data Sources
Cliquez sur `Add data source`
Choisissez Prometheus
Dans le champ URL, mettez :http://prometheus:9090

``` bash
 output
 Successfully queried the Prometheus API.
Next, you can start to visualize data by building a dashboard, or by querying data in the Explore view.
```

Ã‰tape 3 : Importer un Dashboard Node Exporter + FastAPI
ğŸ”¹ Option A : Dashboard Node Exporter (prÃªt Ã  lâ€™emploi)
Menu gauche â†’ ğŸ“Š Dashboards â†’ New Dashboards â†’ ADD visualisation â†’ choisir la source de donnÃ©es prometheus 
Menu gauche â†’ ğŸ“Š Dashboards â†’ New Dashboards â†’ ADD visualisation â†’ choisir la source de donnÃ©es prometheus â†’ 

