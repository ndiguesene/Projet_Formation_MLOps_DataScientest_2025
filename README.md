# Phase 1
 
- **D√©finir les objectifs et m√©triques du projet**
- **Mettre en place l'environnement de d√©veloppement avec Docker ou de simples environnements virtuels**
- **Cr√©er les premiers pipelines de donn√©es et d'entra√Ænement**
- **Mise en place Git pour le code et premiers tests automatis√©s**
- **Cr√©er une API basique pour le mod√®le**
## Cadrage du Projet MLOps
Cataloguer les produits selon des donn√©es diff√©rentes (textes et images) est important pour les e-commerces puisque cela permet de r√©aliser des applications diverses telles que la recommandation de produits et la recherche personnalis√©e. Il s‚Äôagit alors de pr√©dire le code type des produits √† partir de donn√©es textuelles (d√©signation et description des produits) ainsi que des donn√©es d'images (image du produit).
Le client est le site internet de Rakuten, et plus particuli√®rement les administrateurs de ce site.
### √âtapes pour cadrer le projet MLOps (Rakuten)
1. **Objectifs et Probl√©matique**
   1. Pourquoi mettre en place un pipeline MLOps ? (automatisation, scalabilit√©, fiabilit√©‚Ä¶)
   2. Quels sont les objectifs ?
      1. Construire un mod√®le de classification d‚Äôimages pour cat√©goriser des produits e-commerce.
      2. D√©ploiement du mod√®le deep learning en production.
   3. Quels sont les KPIs pour mesurer la performance du mod√®le et du pipeline MLOps ? (ex. : pr√©cision, latence, co√ªt d‚Äôinf√©rence‚Ä¶)
      4. M√©triques :
         5. Accuracy : √âvaluer la proportion de bonnes pr√©dictions 
         6. F1-score : Prendre en compte le d√©s√©quilibre des classes 
         7. Temps d‚Äôinf√©rence : Mesurer la rapidit√© des pr√©dictions du mod√®le
2. **Donn√©es et Infrastructure**
   1. **Sources des donn√©es** : stock√©s dans HuggingFace: https://huggingface.co/datasets/ndiguesene/ml-datasets-image-rakuten-ecommerce/resolve/main/
      - Donn√©es textuelles (~60 MB)
      - Donn√©es images (~2.5 GB)
   2. **Stockage** : fichiers plats (CSV)
   3. **Nettoyage & Pr√©paration** : Strat√©gie de gestion des donn√©es manquantes et transformation.
   4. **Infrastructure** : √Ä d√©finir.
3. **D√©veloppement du Mod√®le ML**
   1. **Choix des algorithmes** : Deep learning
   2. **Frameworks utilis√©s** : TensorFlow, Scikit-learn
4. **Industrialisation avec MLOps**
   1. **CI/CD pour le ML** : GitHub, GitHub Actions
   2. **Gestion des mod√®les** : MLflow
   3. **Orchestration** : Airflow
   4. **Monitoring & Observabilit√©** : Prometheus, Grafana
5. **D√©ploiement et Scalabilit√©**
   1. **Mode de d√©ploiement** : Batch (REST API)
   2. **Infrastructure de d√©ploiement** : Docker, Kubernetes (√† confirmer)
   3. **Gestion du drift** : Retrain automatique, A/B testing, monitoring des performances
6. **S√©curit√© et Gouvernance**
   1. S√©curis√© les APIs expos√©es
## Mettre en place l'environnement de d√©veloppement reproductible
### Cr√©ation de l'environnement 
#### - Sans Docker
Cr√©ez l'environnement virtuel avec les √©tapes suivantes sur Linux :
 
`python -m venv rakuten-project-mlops`
 
Cela va cr√©er un dossier `rakuten-project-mlops/` contenant l‚Äôenvironnement virtuel.
 
### Installer les d√©pendances
 
Installez les d√©pendances n√©cessaires :
 
`pip install -r requirements.txt`
 
#### - Avec Docker
- Cr√©er un Dockerfile et un `docker-compose.yml`
- Construire l‚Äôimage avec `docker build -t ml_env .`
- Lancer le conteneur avec `docker-compose up`
### Importer les donn√©es dans `raw/data`
 
Ensuite, ex√©cutez le script pour importer les donn√©es depuis le repo HuggingFace :
`python3 src/data/import_raw_data.py`
 
Arborescence des donn√©es
```
    data/raw/
    ‚îÇ‚îÄ‚îÄ image_train/
    ‚îÇ   ‚îú‚îÄ‚îÄ image_123456789_product_987654321.jpg
    ‚îÇ   ‚îú‚îÄ‚îÄ image_987654321_product_123456789.jpg
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îÇ‚îÄ‚îÄ image_test/
    ‚îÇ   ‚îú‚îÄ‚îÄ image_111111111_product_222222222.jpg
    ‚îÇ   ‚îú‚îÄ‚îÄ image_222222222_product_111111111.jpg
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
```
---
## Copiez les donn√©es brutes dans `data/preprocessed/` :
 
`python3 src/data/make_dataset.py data/raw data/preprocessed`
 
## Entra√Ænez les mod√®les sur l‚Äôensemble de donn√©es et enregistrez-les dans le dossier models
 
`python3 python src/main.py`
## D√©marrer les pratiques de versionning et des tests automatis√©s
### Suivi et gestion des versions des donn√©es et des mod√®les de donn√©es
1. **Initialiser DVC dans le projet**
```bash
  dvc init
```
2. **Ajouter les fichiers volumineux avec DVC**
```bash
    dvc add data/raw/image_train
    dvc add data/raw/image_test
    dvc add data/preprocessed/X_train_update.csv
    dvc add data/preprocessed/X_test_update.csv
```
3. **Cr√©er un fichier `.gitignore` pour ignorer les fichiers volumineux suivis par DVC**
```bash
  echo "*.dvc" > .gitignore
```
### Configurer un stockage distant avec DVC
On utilisera **S3 d‚ÄôAWS** pour sauvegarder les donn√©es. Pour cela, il faut d'abord installer S3 √† l‚Äôaide de la commande suivante :

 
`pip install "dvc[S3]"`
 
 
Une fois S3 install√©, il faudra mettre √† jour le fichier de configuration `.dvc/config` pour ajouter DagsHub comme notre stockage distant.
---
## Impl√©menter une API basique

PYTHONPATH=$(pwd) pytest tests/test_api.py



---
## Impl√©menter une API basique


## Partie 1 : serving, dockerisation et tests unitaires
### Restructuration des r√©pertoires et fichiers

#### train
Ici, nous entra√Ænons le mod√®le (contient les fichiers main.py et train_model.py). Le r√©pertoire contiendra un Dockerfile sp√©cifique pour lancer l'entra√Ænement du mod√®le √† la demande. Les r√©sultats seront enregistr√©s dans `models` √† la racine
#### predict 
Ici, on applique le mod√®le au donn√©es d'entra√Ænement (contient le fichier predict.py). Le r√©pertoire contiendra un Dockerfile sp√©cifique pour lancer le test du mod√®le sur les donn√©es d'entra√Ænement; Les r√©sultats seront enregistr√©s sur data/predictions √† la racine
#### evaluate 
Ici, on applique le mod√®le au donn√©es de test (devra √™tre cr√©√© mais peut s'inspirer du fichier predict.py). Il contiendra un Dockerfile sp√©cifique pour lancer le test du mod√®le sur les donn√©es de tes; Les r√©sultats seront enregistr√©s sur data/predictions √† la racine. 
##### Il semble que l'on ne puisse pas √©valuer car il n'y a pas de donn√©es lab√©lis√©es sur l'ensemble de test.
#### serve
Ici, on utilisera dans un premier temps requests ou FastAPI pour cr√©er une API qui va interroger le mod√®le avec des donn√©es pr√©alablement renseign√©es (fichier serve.py √† cr√©er) et retournera des r√©sultats que l'on pourra mettre dans data/predictions √† la racine avec un nom de fichier sp√©cifique
Niveaux de serving : 
1. Usage de FastAPI pour cr√©er un contneur basique de serving
2. Usage de BentoML pour automatiser le d√©ploiement, serving etc
#### Variables d'environnement : vous devez rajouter un fichier .env sur la racine du projet
Les chemins des mod√®les (poids, mapper etc) ne sont plus cod√©s en dur. Les modifications suivantes ont √©t√© apport√©es : 
1. ajout fichier `.env` contenant les variables d'environnement (**ce fichier ne doit pas √™tre pouss√© sur le git, il peut renfermer des informations sensibles tels cl√©s d'api etc**)
2. ajout fichier `.env.example` sur le git : fichier d'exemple montrant comment renseigner le fichier .env ( quelles sont les variables d'environnement etc)

## Conteneurisation
Docker est utilis√© pour conteneuriser les diff√©rents services de l'application. Chaque service dispose d'un r√©pertoire sp√©cifique sur les sources.

### Variables d'environnement
Le fichier `.env` √† la racine du projet, doit √™tre mis en jour avec les r√©pertoires de volume cr√©√©s dans le conteneur (Exemple : `TOKENIZER_CONFIG_PATH=/app/models/tokenizer_config.json`).

### Services applicatifs
1. Service de donn√©es
Ici, le service effectue le t√©l√©chargement des donn√©es depuis le d√©p√¥t HuggingFace : `https://huggingface.co/datasets/ndiguesene/ml-datasets-image-rakuten-ecommerce/resolve/main/`.

`Package` : src/data/build_data/

2. Service d'entra√Ænement

`Package` : src/models/train
Contient les fichiers : 
- .dockerignore : fichier pr√©cisant les fichiers et r√©pertoires √† ignorer lors de la construction de l'image
- Dockerfile : commandes pour cr√©er l'image Docker pour ce service uniquement
- main.py : script principal contenant la logique d'entrainement global 
- train_model.py  : script contenant la logique d'entrainement des mod√®les
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les d√©pendences requis pour cr√©er l'image 

3. Service de test

Ici, nous utilisons les donn√©es d'entra√Ænement pour tester l'usage du mod√®le.
`Package` : src/models/predict
Contient les fichiers : 
- .dockerignore : fichier pr√©cisant les fichiers et r√©pertoires √† ignorer lors de la construction de l'image
- Dockerfile : commandes pour cr√©er l'image Docker pour ce service uniquement
- predic.py : script principal contenant la logique de test du mod√®le en utilisant le mod√®le entra√Æn√© et sauvegard√© sur le service d'entra√Ænement
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les d√©pendences requis pour cr√©er l'image 

4. Service de serving

Ici, nous cr√©ons une application permettant d'int√©rroger le mod√®le via API en utilisant FastAPI.
`Package` : src/models/serve
Contient les fichiers : 
- .dockerignore : fichier pr√©cisant les fichiers et r√©pertoires √† ignorer lors de la construction de l'image
- Dockerfile : commandes pour cr√©er l'image Docker pour ce service uniquement
- predic_logic.py : wrapper cr√©√© pour contenir les fonctions requis pour effectuer les pr√©dictions (traitement de l'image, traitement des donn√©es textuelles)
- serve_model_fastapi.py : script principal contenant l'application FastAPI (les endpoints de pr√©diction)
- up.sh : script permettant de lancer un conteneur docker pour ce service uniquement. A lancer depuis la racine du projet.
- requirements.txt : contient les d√©pendences requis pour cr√©er l'image

### API de pr√©diction
Le service de pr√©diction est compos√© pour le moment de deux endpoints : 
- `/status` : pour obtenir le statut de l'application. Retourne un message si l'application est up.
- `/predict` :  pour obtenir une classifiction d'un code produit en fonction des informations suivantes : </br>
`product_identifier`: str = Form(...), # Un identifiant entier pour le produit. Cet identifiant est utilis√© pour associer le produit √† son code de type de produit correspondant. </br>
`designation`: str = Form(...), # Le titre du produit, un court texte r√©sumant le produit </br>
`description`: str = Form(...),  # Un texte plus d√©taill√© d√©crivant le produit. Tous les commer√ßants n'utilisent pas ce champ, donc, afin de conserver l‚Äôoriginalit√© des donn√©es, le champ description peut contenir des valeurs NaN pour de nombreux produits. </br>
`product_id`: str = Form (...), # Un identifiant unique pour ce produit </br>
`imageid`: str = Form (...), # Un identifinat unique de l'image associ√© √† ce produit. </br>
`image`: UploadFile = File(...)  # L'image correspondant au produit </br>

![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_input.png)
![Retour /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_return.png)s

### Services : s√©curisation et logging

##### D√©couplage en services
Chaque fonctionnalit√© du syst√®me (t√©l√©chargement et traitement des donn√©es, entra√Ænement du mod√®le concat√©n√©, test du mod√®le entra√Æn√© et serving) est d√©fini en service respectivement situ√©s dans les r√©pertoires : src/data, src/models/train, src/models/predict, src/models/serve.
Chaque service est utilisable de fa√ßon d√©coupl√© via des conteneurs Docker ainsi que tous les packages requis pour son bon fonctionnement.

##### Limitation taux de requ√™te
La limitation du d√©bit permet d'√©viter les abus et de garantir une utilisation √©quitable de l'API.
La limite de 10 requ√™tes par minutes sur /predict est appliqu√©e.
Un gestionnaire d'exception global pour RateLimitExceeded garantit que les utilisateurs re√ßoivent un message d'erreur clair lorsque les limites sont d√©pass√©es.

##### Authentification et authorisation
Nous avons mis en ≈ìuvre OAuth2 avec JWT pour s√©curiser les points de terminaison. Les cl√©s d'authenication sont tous sauvegard√©s comme variables d'environnement.
Le point de terminaison `/token` g√©n√®re des jetons d'acc√®s, et la d√©pendance `get_current_user` garantit que seuls les utilisateurs authentifi√©s peuvent acc√©der aux points de terminaison prot√©g√©s comme `/predict`.
La protection des points de terminaison sensibles comme `/predict` garantit que seuls les utilisateurs autoris√©s peuvent acc√©der √† vos mod√®les. 
La logique de s√©curit√© est s√©par√©e dans security_logic.py, ce qui rend la base de code plus modulaire et r√©utilisable.
Dans cette version, les utilisateurs ne sont pas dans une base de donn√©es.
- `security_logic.py` : wrapper contenant les logiques d'authentification et d'authorisation utilis√©s par le script principal.

##### Logging
Nous avons mis en place un intergiciel qui enregistre chaque demande avec un identifiant unique, une m√©thode, une URL, un code d'√©tat et un temps de traitement.
La journalisation est essentielle pour le d√©bogage, la surveillance et l'audit.
L'inclusion des identifiants des requ√™tes et des temps de traitement facilite le suivi et l'optimisation des performances. 
- `import_data_logger.log`
- `train_model_logger.log`
- `test_model_logger.log`
- `serving_logger.log`

Nous avons inclu un (RotateFileHandler) pour g√©rer la limitation de la taille des fichiers de logs.

##### Chargement des mod√®les au d√©marrage
Nous avons rajout√© une m√©thode au d√©marrage de l'application de pr√©diction pour charger tous les mod√®les et fichiers de configuration pour √©viter leur chargement √† chaque demande de pr√©diction.

##### Orchestration
Le fichier `docker-compose.xml` √† la racine du projet est utilis√© pour orchestrer tous les services du projet. Nous utilisons le fichier .env pour assurer la r√©usabilit√© des diff√©rents environnemenst (production, staging etc) et la modularit√©.

##### Technologies utilis√©es
1. Docker : utilis√© pour cr√©er chaque service de l'application en un conteneur ind√©pendant
2. Logging et FileHandler : nous utilisons un logging centralis√© pour chaque service (t√©l√©chargement et traitement des donn√©es, entra√Ænement du mod√®le concat√©n√©, test du mod√®le entra√Æn√© et serving).
Chaque service retourne un fichier log sp√©cifique disponible dans le r√©pertoire logs/ √† la racine du projet.
3. Slowapi : utilis√© pour limiter le taux de requ√™te sur les routes.
4. Oauth2 et JWT : s√©curisation des routes d'API en terme d'authentification et d'authorisation.
5. bcrypt : encryption des informations utilisateurs (mots de passe) pour comparaison et validation des informations

##### Usage : 

- Demander un token : `/token`
`curl -X POST "http://127.0.0.1:8000/token" -d "username=user1&password=43SoYourAreADataEngineer34"`
Exemple retours : 
{
    "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
    "token_type": "bearer"
}
- Utiliser le token pour int√©rroger l'API de pr√©diction : `/predict_code`

![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/securing_apis/reports/securing_api_authorized.png)

### Suivi du mod√®le existant best_lstm_model.h5 avec MLflow
Projet_Formation_MLOps_DataScientest_2025/
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ ‚îú‚îÄ‚îÄ best_lstm_model.h5
‚îÇ ‚îú‚îÄ‚îÄ tokenizer_config.json
‚îÇ ‚îú‚îÄ‚îÄ mapper.pkl
‚îÇ ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îú‚îÄ‚îÄ log_existing_model.py
‚îî‚îÄ‚îÄ README.md

## ‚úÖ Objectif

Logger le mod√®le d√©j√† entra√Æn√© (`best_lstm_model.h5`) dans MLflow pour :
- le sauvegarder comme artefact,
- ajouter des m√©tadonn√©es (param√®tres, tags),
- le visualiser depuis l'interface web de MLflow,
- faciliter sa r√©utilisation (rechargement ou d√©ploiement).

## ‚öôÔ∏è Pr√©-requis

- MLflow install√© :
  ```bash
  pip install mlflow tensorflow

  1. Lancer le tracking server (facultatif si en local)
 ```bash

   mlflow ui --host 0.0.0.0 --port 5000
```

## Contenu de ``log_existing_model.py``
# Ce script permet de logger le mod√®le dans MLflow.

``` bash
import mlflow
import mlflow.keras
from tensorflow.keras.models import load_model

model = load_model("models/best_lstm_model.h5")

mlflow.set_tracking_uri("file:///home/ubuntu/projet_mlops/Projet_Formation_MLOps_DataScientest_2025/mlruns")

with mlflow.start_run(run_name="Log_Existing_LSTM_Model"):
    mlflow.log_param("type", "pretrained")
    mlflow.log_param("format", "H5")
    mlflow.keras.log_model(model, "lstm_model_logged")
```
    
# R√©sultats attendus 
Visualisation des m√©triques et des artefacts sur l'interface web de MLflow :

![Mlfow / Experiment](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/test_mmlflow/reports/mlfow_experiment.png)

![Mlfow / Metris](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/test_mmlflow/reports/mlflow_metrics.png)s



## Proc√©dure pour int√©grer MLflow avec DagsHub
Afin de visualiser les exp√©riences dans l'onglet "MLflow" du d√©p√¥t DagsHub, le fichier ``log_with_mlflow.py`` est une version √©tendue qui :

- Initialise l'int√©gration avec DagsHub,

- Logue le mod√®le et ses artefacts dans MLflow,

- Propulse toutes les donn√©es sur DagsHub, en liant DVC, Git et MLflow


# Initialisation de DagsHub avec l'int√©gration de MLflow
le script `log_with_mlflow.py`

``` bash
import dagshub
dagshub.init(repo_owner='mariamanadia',
             repo_name='Projet_Formation_MLOps_DataScientest_2025',
             mlflow=True)

import mlflow
with mlflow.start_run():
  mlflow.log_param('parameter name', 'value')
  mlflow.log_metric('metric name', 1)

```
# Installation des d√©pendances
Installer MLflow et DagsHub 
``` bash
pip install mlflow dagshub

```

Fonctionnement
DVC : G√®re le versioning des donn√©es et des mod√®les.

Git : Gestion du code source, suivi des versions.

MLflow : Suivi des exp√©riences, gestion des mod√®les, visualisation des r√©sultats.

DagsHub : Int√©gration de DVC, Git et MLflow pour centraliser la gestion de ton projet MLOps.

Une fois le script ex√©cut√©, tu pourras voir l'exp√©rience dans l'interface web de MLflow ainsi que dans ton d√©p√¥t DagsHub.

 MLFLOW sur Dags
![MLFLOW / DAGSHUB](https://dagshub.com/MariamaNadia/Projet_Formation_MLOps_DataScientest_2025/experiments)

# Pourquoi ne pas utiliser dvc add pour le mod√®le ?
Le mod√®le best_lstm_model.h5 est d√©j√† g√©r√© automatiquement par DVC via le pipeline d√©fini dans ``dvc.yaml`` 
``` bash
stages:
  train:
    cmd: python src/models/train/train_model.py
    deps:
      - data/raw
      - src/models/train/train_model.py
    outs:
      - models/best_lstm_model.h5

````
Cela permet √† DVC de suivre automatiquement les versions du mod√®le g√©n√©r√© sans utiliser dvc add.
Utiliser dvc add en plus provoquerait un conflit de suivi.

# Test Unitaire
## Documentation des tests unitaires

Ce document d√©crit les tests unitaires d√©finis pour l‚ÄôAPI FastAPI. Ils couvrent les trois endpoints principaux : racine (`/`), `GET /status` et `POST /predict`.

---

## Pr√©requis

- Python 3.9
- FastAPI
- Uvicorn (pour lancer l‚ÄôAPI si besoin)
- pytest
- httpx (install√© automatiquement avec FastAPI)

Installer les d√©pendances :

```bash
  pip install fastapi uvicorn pytest httpx
```

## Lancer les tests

Depuis la racine du projet (l√† o√π se trouve src/ et tests/), ex√©cuter :

```bash
   pytest tests/test_api.py
```
## Structure des tests

Le fichier tests/test_api.py contient trois fonctions :
```python
from fastapi.testclient import TestClient
from Projet_Formation_MLOps_DataScientest_2025.api import app

client = TestClient(app)
```
Chaque test utilise un TestClient pour simuler des requ√™tes HTTP vers l‚Äôapplication FastAPI.

1. **test_status**
   2. Objectif : v√©rifier que l‚Äôendpoint GET /status renvoie bien un code HTTP 200 et le message attendu. 
   3. script
      ```python
      def test_status():
          response = client.get("/status")
          assert response.status_code == 200
          assert response.json() == {"status": "L'API fonctionne correctement"}
      ```
   3. Requ√™te : 
   ```http request
   GET /status
   ```
   4. Assertions :
      - `response.status_code == 200`
      - `response.json() == {"status": "L'API fonctionne correctement"}`

   2. **test_root**
      3. Objectif : valider que la racine / renvoie un message de bienvenue.
      4. script
      ```python
      def test_root():
          response = client.get("/")
          assert response.status_code == 200
          data = response.json()
          assert "message" in data
          assert "Bienvenue" in data["message"]
         ```
      4. Requ√™te :
      ```http request
      GET /
      ```
      5. Assertions
         - `response.status_code == 200`
         - `Le corps JSON contient une cl√© message`
         - `La cha√Æne "Bienvenue" appara√Æt dans la valeur de message`

3. **test_predict**
   4. Objectif : s‚Äôassurer que l‚Äôendpoint POST /predict accepte un payload JSON arbitraire et retourne un JSON avec les cl√©s predicted et label.
   5. script
   ```python
   def test_predict():
       sample_text = {"predicted": "1", "label": "label_1"}
       response = client.post("/predict", json=sample_text)
       assert response.status_code == 200
       data = response.json()
       assert "predicted" in data
       assert "label" in data
   ```
   5. Requ√™te :
   ```http request
   POST /predict
   Content-Type: application/json
   {"predicted": "1", "label": "label_1"}
   ```
   6. Assertions :
    - response.status_code == 200 
    - La r√©ponse JSON contient les champs predicted et label

---
## Automatisation DVC/DgasHub/MLFlow (√† mettre √† jour par Awa TIAM)
Note : √† partir de l√†, tout se fait via les conteneurs Docker. Donc, il faut obligatoirement disposer d'un fichier .env √† la racine du projet. Exemple de contenu .env partag√© via Teams.
#### Cr√©er un fichier .env (ne pas oublier de mettre √† jour les infos dagshub (token))
```bash
touch .env

```

#### Donn√©es n√©cessaires : fichiers artefacts non dispoinibles sur Git
Les fichiers .h5, .json, .pkl (best_lstm_model, best_vgg16, best_weights, mapper, tokenizer) doivent √™tre disponibles dans le dossier `models`.
Ils ont utilis√© dans l'entra√Ænement des mod√®les.
Ces fichiers sont partag√©s via Teams.

#### Ex√©cuter la pipeline sans Airflow

```bash
docker compose up

```

!! Le fichier √† la racine `run_compose_options.sh` contient des examples de commandes pour ex√©cuter chaque service en particulier.

#### Ex√©cuter la pipeline via Airflow

##### Mettre √† jour le fichier sous airflow/dags/initialize_airflow.sh et ex√©cuter le fichier

```bash
export AIRFLOW_HOME=`r√©pertoire absolu du projet`/airflow  
airflow db migrate

# Start Airflow in standalone mode (creates an admin user automatically, crdentials are saved /Users/tiam028713/airflow/simple_auth_manager_passwords.json.generated )
airflow standalone

```


##### Co

# Start Airflow in standalone mode (creates an admin user automatically, crdentials are saved /Users/tiam028713/airflow/simple_auth_manager_passwords.json.generated )
airflow standalone

#### Ports ouverts

Le service d'authentification est ouvert sur le port 8011:8011
Le service d'API est ouvert sur le port 8000:8000


###  Monitoring

Int√®grer un syst√®me de monitoring complet bas√© sur **Prometheus** et **Node Exporter**, avec exposition des m√©triques de l'application **FastAPI**.

## 1.  Installation des d√©pendances

Ajout de la d√©pendance Prometheus dans le fichier `requirements.txt` :

```
prometheus-fastapi-instrumentator
```

Installation avec pip :

```bash
pip install prometheus-fastapi-instrumentator
```

## 2.  Modification du code FastAPI

Ajout de l‚Äôinstrumentation Prometheus dans le fichier `src.models.serve.serve_model_fastapi.py` :

```python
from prometheus_fastapi_instrumentator import Instrumentator

# Ajout apr√®s la cr√©ation de l'application FastAPI
app = FastAPI()

# Instrumentation Prometheus
Instrumentator().instrument(app).expose(app)

```
## 3. Configuration prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]

  - job_name: "fastapi"
    static_configs:
      - targets: ["localhost:8000"]

  - job_name: "node"
    static_configs:
      - targets: ["localhost:9100"]


## 4.  Lancement de l‚ÄôAPI , Prometheus et node exporter

```bash
uvicorn src.models.serve.serve_model_fastapi:app --reload
```

```bash
prometheus --config.file=prometheus.yml
```

```bash
cd node_exporter-1.8.1.linux-amd64
./node_exporter &
```

## 5.  V√©rification de l‚Äôendpoint /metrics , prometheus, node exporter

Ouvrir dans le navigateur ou via curl :

```bash
http://localhost:8000/metrics

```
- > ![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_input.png)

```bash
//http://localhost:9090

```
- > ![Endpoint /predict](https://github.com/ndiguesene/Projet_Formation_MLOps_DataScientest_2025/blob/awa/restructure_folders/reports/predict_endpoint_input.png)

```bash
http://http://localhost:9100
```

## 6. M√©triques collect√©es (exemple) :
- `http_requests_total{job="fastapi"}` ‚Äî nombre de requ√™tes HTTP par endpoint, m√©thode, code
- `process_cpu_seconds_total` ‚Äî temps CPU utilis√©
- `node_memory_Active_bytes` ‚Äî RAM active

### ‚Üí Installer et configurer Grafana, le connecter √† Prometheus et visualiser les m√©triques (FastAPI, Node Exporter) avec des dashboards.
√âtape 1 : Lancer Grafana avec Docker

``` bash
docker run -d -p 3000:3000 --name=grafana grafana/grafana
```
√âtape 2 : Ajouter Prometheus comme source de donn√©es
Acc√®de √† Grafana : http://localhost:3000
Menu lat√©ral gauche ‚Üí ‚öôÔ∏è Configuration ‚Üí Data Sources
Clique sur Add data source
Choisis Prometheus
Dans le champ URL, mets :http://localhost:9090

``` bash
 output
 Successfully queried the Prometheus API.
Next, you can start to visualize data by building a dashboard, or by querying data in the Explore view.
```
√âtape 3 : Importer un Dashboard Node Exporter + FastAPI
üîπ Option A : Dashboard Node Exporter (pr√™t √† l‚Äôemploi)
Menu gauche ‚Üí üìä Dashboards ‚Üí Import

Dans le champ "Import via grafana.com", entre l‚ÄôID suivant :
1860